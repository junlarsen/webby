---
sidebar_position: 1
slug: /tdt4120-algoritmer-og-datastrukturer
---

# TDT4120: Algoritmer og datastrukturer

Tatt h√∏sten 2022. Nedenfor er notater fra alle forelesningene bortsett fra forelesning 14.

## 01. Problemer og algoritmer

### üìöForkunnskaper og pseudokode

Forkunnskaper til emner er logaritmer, spesielt logaritmisk notasjon og regneregler for logaritmer slik at det er enklere √• se at teoremer som masterteoremet g√•r opp. Det er i tillegg n√∏dvendig √• kjenne til modulo operatoren som er resten av en br√∏k.

L√¶reboka er CLRS ([https://en.wikipedia.org/wiki/Introduction_to_Algorithms](https://en.wikipedia.org/wiki/Introduction_to_Algorithms)) 4. edition. Boka har g√•tt igjennom betydelige endringer siden 3. edition og anbefales dermed ikke. Pseudokoden i boka antar i de fleste tilfeller at indeksing av lister begynner p√• 1.

### üìöProbleml√∏sing og dekomponering

Det finnes mange problemer som datamaskiner kan l√∏se, men ikke alle l√∏sninger til problemene fungerer, spesielt ikke p√• stor skala.

> Eksempel fra forelesning: hvis vi skulle brute-force matching mellom donor og mottaker s√• hadde vi kun matchet i overkant av hundre personer p√• 10000000‚Ä¶ ganger universets levetid. Dette er estimert til √• ta en halvtime for alle menneskene p√• kloden med Chandran-Hochbaum algoritmen.

Det er derfor viktig at vi bruker en mer optimal l√∏sning for √• l√∏se problemet. Hvis vi bruker en ordentlig algoritme istedenfor √• brute-force problemet kan vi n√• realistiske kj√∏retider.

Et problem kan beskrives som en general relasjon mellom input og output. Begrepet for en enkel, konkret input er en instans. Siden algoritmer ofte jobber p√• en eller annen type datastruktur som en liste eller et tre kan det v√¶re nyttig √• dekomponere problemet i mindre problemer (instanser).

P√• denne m√•ten kan vi f√∏lge en metode lik matematisk induksjon for √• skape l√∏sninger for problemene. Vi bryter problemet ned til et ‚Äúbase case‚Äù som vi kan bevise at stemmer. Deretter bygger vi opp l√∏sningen ved hjelp av induksjon.

![Untitled ](01/Untitled%201.png)

![Untitled ](01/Untitled%202.png)

![Untitled ](01/Untitled%203.png)

For √• gj√∏re det enklere √• bryte opp problemet kan vi bruke f√∏lgende tabell for √• f√• en fin mental oversikt over problemet vi √∏nsker √• l√∏se. F√∏rst fyller vi ut ‚ÄúInstans‚Äù og ‚ÄúL√∏sning‚Äù som tilsvarer input og output til problemet. Deretter jobber vi oss nedover p√• vestre siden og til slutt g√•r vi opp p√• h√∏yre side og l√∏ser resten av problemet. Nedenfor er eksempel for l√∏sing av insertion-sort.

![Untitled ](01/Untitled%204.png)

![Untitled ](01/Untitled%205.png)

I kode har vi to m√•ter √• l√∏se slike rekursive induksjonsproblemer; l√∏kker og rekursjon. En l√∏kkeinvariant er et delsteg i l√∏sningen for en iterasjon av l√∏sningen til problemet. Vi beviser at l√∏sningen v√•r er rett for grunntilfellet, og gjennom hvert steg i l√∏kkeinvarianten, beviser at dell√∏sningen er rett b√•de f√∏r og etter. P√• denne m√•ten kan vi induktivt bevise at sluttl√∏sningen v√•r er riktig. Om dette gj√∏res ved en faktisk l√∏kke eller rekursjon har ikke noe √• si. Som alltid, m√• vi bevise at l√∏kka eller rekursjonen stopper.

### üíªInsertion Sort

Insertion Sort er en sorteringsalgoritme som fungerer ved √• ta hvert element i lista, og flytte det mot venstre helt til det ligger i riktig posisjon i den sorterte lista. Insertion Sort er en stabil sorteringsalgoritme som betyr at den beholder rekkef√∏lgen p√• like elementer.

- Best case: $O(n)$ hvis lista er sortert fra f√∏r av
- Worst case: $O(n^2)$ hvis lista er sortert fra h√∏yt til lavt (revers)
- Average case: $O(n^2)$

[algdat/InsertionSort.scala at main ¬∑ matsjla/algdat](https://github.com/matsjla/algdat/blob/main/src/main/scala/com/supergrecko/dsa/sorting/InsertionSort.scala)

### üìöAsymptotisk notasjon

Asymptotisk notasjon er notasjon som ofte brukes for √• vise hvor ressurseffektiv en algoritme eller l√∏sning er. Man ser ofte p√• kj√∏retid, men man kan ogs√• ha asymptotisk notasjon av minnebruk, instruksjonssykluser og liknende data. Nedenfor er skalaen p√• de forskjellige formene.

![Untitled ](01/Untitled%206.png)

N√•r vi bruker asymptotisk notasjon er vi kun interessert i en veldig grov st√∏rrelsesorden. Dermed dropper vi konstanter og lavere ordens ledd.

$$
\Theta(n^2 + n + 10) = \Theta(n^2)
$$

Reell eller n√∏yaktiv kj√∏retid kan v√¶re varierende eller udefinert. Vi √∏nsker dermed √• finne en form som likner kurven. Vi bruker tre forskjellige m√•linger i asymptotisk notasjon. Det er viktig √• vite at det finnes ingen direkte sammenheng mellom $O(n)$, $\Theta(n)$, og $\Omega(n)$ og best/worst/average case kompleksitet for en algoritme. Asymptotisk notasjon er bare en m√•te √• definere best/worst/average case.

- $O(n)$ eller big-o definerer den √∏vre grensa. Dette betyr at funksjonen kan v√¶re opptil $n$ i kompleksitet, men ogs√• veldig mye mindre.
- $\Theta(n)$ eller stor theta definerer ‚Äútightest bound‚Äù. Dette betyr at funksjonen vil v√¶re tiln√¶rmet $n$. Det impliserer ogs√• $O(n) = \Omega(n) \implies \Theta(n)$
- $\Omega(n)$ eller stor omega definerer den nedre grensa. Dette betyr at funksjonen har ihvertfall $n$ i kompleksitet, men det er ingen √∏vre grense som betyr at den kan v√¶re uendelig kompleks.

I tillegg har vi $o(n)$ og $\omega(n)$ som er √∏vre, men ikke lik, og nedre, men ikke lik grenser.

N√•r en skal regne p√• asymptotisk notasjon hjelper det √• tenke p√• at notasjonen $O(n)$ definerer mengden av alle funksjonene med kompleksitet $n$ eller mindre. Det resulterer ofte iat $\Omega(n)$ dominerer et uttrykk. Dessuten er $f(n) = O(n)$ notasjon misbruk da $O(n)$ er en mengde.

![Untitled ](01/Untitled%207.png)

---

## 02. Datastrukturer

### üíªStack

En Stack er en line√¶r last-in-first-out (LIFO) datastruktur. En stack fungerer som en stabel hvor elementene som legges p√• f√∏rst, kommer ut sist. Relevant funksjonalitet p√• en Stack i pensum er:

- $StackEmpty$ - er stacken tom?
- $StackPush$ - legg til et element p√• toppen av stacken i konstant tid $O(1)$
- $StackPop$ - fjern et element fra toppen av stacken i konstant tid $O(1)$

[algdat/Stack.scala at main ¬∑ matsjla/algdat](https://github.com/matsjla/algdat/blob/main/src/main/scala/com/supergrecko/dsa/linear/Stack.scala)

### üíªQueue

En Queue er en line√¶r first-in-first-out (FIFO) datastrukturo. En Queue fungerer p√• lik m√•te som en Stack, bortsett fra at elementene som ble lagt p√• f√∏rst, kommer ut f√∏rst. Relevant funksjonalitet p√• en Queue fra pensum er:

- $QueueEnqueue$ - legg til et element i k√∏en i konstant tid $O(1)$
- $QueueDequeue$ - ta det fremste elementet i k√∏en i konstant tid $O(1)$
- $QueueSize$ - st√∏rrelsen p√• k√∏en, som regel kun relevant hvis k√∏en ikke backes av en resizable container.

Implementasjonen i pseudokode i pensum tar ikke hensyn til overflow eller underflow av k√∏en. Implementasjon i Scala tar hensyn til dette.

[algdat/Queue.scala at main ¬∑ matsjla/algdat](https://github.com/matsjla/algdat/blob/main/src/main/scala/com/supergrecko/dsa/linear/Queue.scala)

### üíªLinkedList

En lenket liste (i pensum, doubly-linked-list) er en liste som best√•r av noder som er allokert tilfeldige steder i minnet som er koblet sammen via pekere. Det tar dermined line√¶r tid √• sl√• opp p√• en gitt posisjon, men det er konstant tid for √• sette inn eller slette elementer. Relevant funksjonalitet p√• en LinkedList fra pensum er:

- $ListSearch$ - s√∏k igjennom en liste. Beste tilfelle p√• $O(1)$ hvis f√∏rste element er s√∏keelementet, ellers er det gjennomsnittslig og verste tilfelle p√• $O(n)$.
- $ListPrepend$ - legg til en node i frontend av lista i konstant tid $O(1)$.
- $ListInsert$ - legg til en ny node $x$ foran en eksisterende node $y$ i konstant tid $O(1)$. Denne funksjonen trenger ikke et liste objekt da den kun operer p√• to noder.
- $ListDelete$ - slett en node $x$ fra lista i konstant tid $O(1)$

[algdat/LinkedList.scala at main ¬∑ matsjla/algdat](https://github.com/matsjla/algdat/blob/main/src/main/scala/com/supergrecko/dsa/linear/LinkedList.scala)

### üíªHashTable

En hashtabell er en relasjon eller mapping mellom en n√∏kkel p√• en verdi. En hashtabell implementeres som regel ved hjelp av en line√¶r datastruktur av LinkedList-er. N√∏kkelen g√•r igjennom en hash-funksjon som bestemmer hvilken b√∏tte verdien skal havne i. En god hash-funksjon vil ha s√• f√• kollisjoner som mulig som gj√∏r innsetting til, og henting fra tabellen ta konstant tid.

I tilfelle hvor det er kollisjoner, lagres ogs√• noe ekstra data (gjerne n√∏kkelen eller annen data som kan gjenkjenne n√∏kkelen) for √• l√∏se kollisjoner. Dette resulterer i veldig simple algoritmer for √• bruke hashtabellen. Relevant funksjonalitet p√• et HashTable fra pensum er:

- $ChainedHashSearch$ - finn verdien til en n√∏kkel i hashtabellen. Kj√∏rer i beste og gjennomsnittslig tilfelle i konstant tid $O(1)$, men siden det kan v√¶re kollisjoner er verste tilfelle $O(n)$.
- $ChainedHashInsert$ - sett inn gitt n√∏kkel og verdi i lista i konstant tid $O(1)$.
- $ChainedHashDelete$ - fjern verdien til en n√∏kkel i hashtabellen. Kj√∏rer i beste og gjennomsnittslig tilfelle i konstant tid $O(1)$, men siden det kan v√¶re kollisjoner er verste tilfelle $O(n)$.

[algdat/HashTable.scala at main ¬∑ matsjla/algdat](https://github.com/matsjla/algdat/blob/main/src/main/scala/com/supergrecko/dsa/linear/HashTable.scala)

### üíªDynamicTable

En dynamisk tabell er en liste eller et array som har evnen til √• gro eller minke i st√∏rrelse basert p√• antall elementer i lista. Det betyr at lista kan bli (i teorien) uendelig stor siden vi kan gro lista n√•r den n√¶rmer seg full. Det er vanlig √• bruke en st√∏rre faktor enn $n+1$ for √• gro tabellen for √• unng√• mange allokasjoner. Relevant funskjonalitet p√• et DynamicTable fra pensum er:

- $TableInsert$ - legg til et nytt element i den dynamiske lista. Kj√∏rer beste og gjennomsnittslig tilfelle i konstant tid $O(1)$, men siden lista m√• gro innimellom er verste tilfelle $O(n)$.

[algdat/DynamicTable.scala at main ¬∑ matsjla/algdat](https://github.com/matsjla/algdat/blob/main/src/main/scala/com/supergrecko/dsa/linear/DynamicTable.scala)

### üìöAmortisert arbeid

Kj√∏retiden for √©n enkelt operasjon er ikke alltid like informativt. I en dynamisk tabell kan en insert operasjon f√• lista til √• gro og dermed bli $O(n)$, men siden vi f√•r en st√∏rre grofaktor vil dette som regel ikke skje. Amortisert arbeid er aggregert analyse av et problem. Vi finner totalt arbeid og deler p√• antall operasjoner.

Average-case kommer dermed av forventet kj√∏retid for en algoritme, selv om den kan i noen tilfeller v√¶re mye tregere (som i $DynamicTableInsert$). Amortisert arbeid er snitt over operasjoner.

---

## 03. Splitt og hersk

### üìöSplitt og hersk

Splitt og hersk er et paradigme for √• bryte ned et kompleks problem inn i mindre underproblemer og l√∏se underproblemene for seg selv. Deretter setter vi sammen dell√∏sningene for √• lage en induktivt riktig l√∏sning.

### üìöRekurrenser

Rekurrenser er rekursive likninger, gjerne p√• formen $f(n) = f(n - 1) + c$. Problemet med slike definisjoner er at de er rekursive som betyr at vi kan pr√∏ve √• l√∏se de, men utfordringen er √• komme seg til bunns og deretter n√∏ste seg opp igjen.

![Untitled](03/Untitled%201.png)

Rekurrenser har som regel et grunntilfelle som gj√∏r det enklere √• n√• bunnen.

$$
f(n) = \begin{cases}n = 1 & 1\\n \neq 1 & f(n - 1) + 1\end{cases}
$$

Rekurrenser er nyttige fordi de kan beskrive kompleksiteten eller kj√∏retiden til rekursive algoritmer. Vi kan l√∏se rekurrenser med:

- Iterasjonsmetoden: gjenntatt ekspandering av rekurrensen til vi f√•r en sum vi kan regne ut
- Rekursjonstr√¶r: ekspander rekurrensen og lag et tre man kan summere p√•. TODO
- Masterteoremet: en enkel m√•te √• l√∏se rekurrenser av formen $T(n) = a \cdot T(\frac{n}{b}) + f(n)$. Mer om masterteoremet kommer senere i notatet.

Til slutt verifiserer vi l√∏sningen v√•r med induksjonsbevis.

### üíªBinary Search

Bin√¶rs√∏k er en divide & conquer s√∏kealgoritme som opererer p√• en sortert liste. Den velger elementet i midten av lista og sammenligner det med s√∏keelementet, og kaller seg selv rekursivt p√• enten h√∏yre eller venstreside av lista. Grunntilfellet er en tom liste hvor vi kan konkludere med at elementet ikke eksisterer i lista.

- Best case: $O(1)$ hvis s√∏keelementet er i midten av lista
- Worst case: $O(\lg n)$ siden rekursjonstreet har h√∏yde $\lg n$
- Average case: $O(\lg n)$

[algdat/BinarySearch.scala at main ¬∑ matsjla/algdat](https://github.com/matsjla/algdat/blob/main/src/main/scala/com/supergrecko/dsa/search/BinarySearch.scala)

### üíªMerge Sort

Merge Sort er en divide & conquer sorteringsalgoritme. Den drives av subprosedyren $Merge$ som tar inn en liste, splitter det i to og sorterer sublistene. Siden merge kalles etter de rekursive kallene til $MergeSort$ vil det via induksjon bevises at halvsidene av lista er sortert f√∏r de merges. Grunntilfellet er en tom liste hvor det ikke finnes mer arbeid √• gj√∏re.

- Best case: $O(n \lg n)$
- Worst case: $O(n \lg n)$
- Average case: $O(n \lg n)$

Kompleksiteten blir $O(n \lg n)$ fordi $Merge$ har en kompleksistet p√• $n$, ganget med h√∏yden p√• rekursjonstreet som er $\lg n$.

[algdat/MergeSort.scala at main ¬∑ matsjla/algdat](https://github.com/matsjla/algdat/blob/main/src/main/scala/com/supergrecko/dsa/sorting/MergeSort.scala)

### üíªQuick Sort

Quick Sort er en divide & conquer sorteringsalgoritme. Den drives av subprosedyren $Partition$ som velger et pivotelement som plasseres p√• riktig plass i den sorterte lista, samtidig som den s√∏rger for at alle elementer til venstre er lavere, og alle til h√∏yre er h√∏yere. Siden partition kalles f√∏r det rekursive kallet til $QuickSort$ partisjoneres alle breakpoints i lista og dermed kan vi ved induksjon bevise at lista er sortert. Grunntilfellet er tom liste hvor det ikke finnes mer arbeid √• gj√∏re.

- Best case: $O(n \lg n)$
- Worst case: $O(n^2)$ hvis lista var sortert fra f√∏r av og pivot velges som siste element
- Average case: $O(n \lg n)$

For √• unng√• tilfellet hvor d√•rlig valg av pivot oppst√•r finnes en alternativ m√•te √• velge pivot p√•; randomisert Quick Sort. Her vil verste tilfelle bli $O(n \lg n)$ n√•r $n \to \infty$ ettersom det blir tiln√¶rmet null sjanse for at verste tilfellet oppst√•r.

[algdat/QuickSort.scala at main ¬∑ matsjla/algdat](https://github.com/matsjla/algdat/blob/main/src/main/scala/com/supergrecko/dsa/sorting/QuickSort.scala)

[algdat/RandomizedQuickSort.scala at main ¬∑ matsjla/algdat](https://github.com/matsjla/algdat/blob/main/src/main/scala/com/supergrecko/dsa/sorting/RandomizedQuickSort.scala)

### üìöMasterteoremet

Masterteoremet er en veldig enkel metode for √• l√∏se rekurrenser p√• formen $T(n) = a \cdot T(\frac{n}{b}) + f(n)$. Her gjelder det bare √• vite at $a^{\lg n} = n^{\lg a}$. Ved √• se p√• hvordan logaritmen skisses p√• grafen er det enkelt √• se at disse to er like.

![Untitled](03/Untitled%202.png)

Vi kan bruke iterasjonsmetoden for √• finne et m√∏nster i den rekursive funksjonen. Vi f√•r dermed en l√∏sning p√• formen $a^?T(\frac{n}{b^?})$. Her setter vi inn $? = \lg_b n$.

![Untitled](03/Untitled%203.png)

$$
a^{\lg_b n} \cdot T(\frac{n}{b^{\lg_b n}}) \\
a^{\lg_b n} \cdot T(\frac{n}{n}) \\
a^{\lg_b n} \cdot 1 \\
a^{\lg_b n} \\
n^{\lg_b a}
$$

Til slutt sammenligner vi resultatet med den drivende funksjonen $f(n)$.

![Untitled](03/Untitled%204.png)

---

## 04. Rangering i line√¶r tid

### üìöSortingsgrensen

Det finnes en nedre grense for hvor raske rangeringsalgoritmer kan v√¶re. Denne grensa er p√• $\Omega(n \lg n)$. Dette er synlig i divide-and-conquer algoritmer som deler opp problemet p√• en m√•te som gj√∏r det mulig √• forestille seg et rekursjonstre.

![[https://cs.stackexchange.com/questions/32311/proving-the-lower-bound-of-compares-in-comparison-based-sorting/32312#32312](https://cs.stackexchange.com/questions/32311/proving-the-lower-bound-of-compares-in-comparison-based-sorting/32312#32312)](04/Untitled%201.png)

[https://cs.stackexchange.com/questions/32311/proving-the-lower-bound-of-compares-in-comparison-based-sorting/32312#32312](https://cs.stackexchange.com/questions/32311/proving-the-lower-bound-of-compares-in-comparison-based-sorting/32312#32312)

Likevel finnes det algoritmer som $InsertionSort$ som kj√∏rer i $O(n)$ i beste tilfelle, men det er kun p√• grunn av hvordan algoritmen er skrevet. Dessuten kan vi utnytte sorteringsgrensen, og til og med bryte den ved √• skrive smarte algoritmer. Et eksempel p√• dette er $RandomizedSelect$ som kommer senere. Her er generell tabell for sorteringsgrensa.

|          | Best        | Average           | Worst             |
| -------- | ----------- | ----------------- | ----------------- |
| $O$      | $O(\infty)$ | $O(\infty)$       | $O(\infty)$       |
| $\Theta$ | $\Theta(?)$ | $\Theta(?)$       | $\Theta(?)$       |
| $\Omega$ | $\Omega(n)$ | $\Omega(n \lg n)$ | $\Omega(n \lg n)$ |

Det er umulig √• si noe om den generelle √∏vre grensa ettersom algoritmen kan ta uendelig med tid for alt vi vet. Dessuten er det ikke mulig √• g√• lavere enn $n$ fordi vi m√• garantere at hele sekvensen behandles.

### üìöReduksjonsbevis

Vi kan bevise egenskaper til enkelte problemer ved √• bruke de som del-l√∏sning av et annet problem. Dette kalles reduksjonsbevis. Et eksempel p√• dette er unikhetsproblemet.

![Untitled](04/Untitled%202.png)

I dette problemet blir vi fortalt at man kan i verste tilfelle finne ut om en tabell har duplikater i $\Omega(n \lg n)$ tid. Siden sortering er mesteparten av arbeidet l√∏sningen kan det umulig ha seg at sortering i verste tilfelle er bedre enn $\Omega(n \lg n)$.

![Untitled](04/Untitled%203.png)

> I forelesning 4 presenteres en analogi for reduksjonsbeviset med en l√•st kiste. Det kan umulig v√¶re lettere √• √•pne kista enn det er √• √•pne l√•sen.

### üíªRandomized Select

Randomized Select er en divide & conquer algoritme som effektivt fungerer som ‚ÄúQuickSort som BinarySearch‚Äù. Randomized Select finner elementet p√• $i$-ende indeks i lista. Vi bruker faktum at $Partition$ subprosedyren til $QuickSort$ plasserer pivotelementet p√• riktig plass, og s√∏rger for at elementene til vestre er lavere, og elementene til h√∏yre er h√∏yere. P√• denne m√•ten kan vi splitte lista igjen og finne elementet p√• en gitt indeks.

- Best case: $O(n)$ av rekurrensen $2n - 1$
- Average case: $O(n)$
- Worst case: $O(n^2)$ hvis pivotelementet er alene, slik som i $QuickSort$. Lite sannsynlig til √• skje med randomisering.

I tillegg finnes en annen select algoritme som heter $Select$. Denne er mer kompleks og grundig forst√•else er ikke et krav i pensum.

[algdat/RandomizedSelect.scala at main ¬∑ matsjla/algdat](https://github.com/matsjla/algdat/blob/main/src/main/scala/com/supergrecko/dsa/selection/RandomizedSelect.scala)

### üíªCounting Sort

Counting Sort er en stabil sorteringsalgoritme som bryter sorteringsgrensen ved √• gj√∏re en antakelse at ingen elementer er h√∏yere enn en gitt verdi $k$. Deretter lager den en liste som holder styr p√• hvilken indeks ethvert element i lista skal v√¶re i den endelige lista.

- Best case: $\Theta(n + k)$
- Worst case: $\Theta(n + k)$
- Average case: $\Theta(n + k)$

[algdat/CountingSort.scala at main ¬∑ matsjla/algdat](https://github.com/matsjla/algdat/blob/main/src/main/scala/com/supergrecko/dsa/sorting/CountingSort.scala)

### üíªRadix Sort

Radix Sort er en stabil sorteringsalgoritme som bryter sorteringsgrensen ved √• gj√∏re en antaklese at ingen elementer har flere enn $d$ siffer. Deretter bruker den en stabil sorteringsalgoritme som $CountingSort$ til √• sortere etter hvert siffer.

- Best case: $\Theta(d(n + k)$
- Worst case: $\Theta(d(n + k)$
- Average case: $\Theta(d(n + k)$

[algdat/RadixSort.scala at main ¬∑ matsjla/algdat](https://github.com/matsjla/algdat/blob/main/src/main/scala/com/supergrecko/dsa/sorting/RadixSort.scala)

### üíªBucket Sort

Bucket Sort er en sorteringsalgoritme som bryter sorteringsgrensen ved √• anta at alle verdier er flyttall mellom $[0, 1)$ p√• en uniform distribusjon. Den deler elementene opp i $n$ b√∏tter, og siden distribusjonen skal v√¶re uniform kommer det ikke til √• v√¶re mange tall i hver b√∏tte. Dermed sl√•r den sammen alle listene. Bucket Sort er stabil hvis sorteringsalgoritmen den bruker er stabil.

- Best case: $O(n)$
- Worst case: $O(n^2)$
- Average case: $O(n)$

[algdat/BucketSort.scala at main ¬∑ matsjla/algdat](https://github.com/matsjla/algdat/blob/main/src/main/scala/com/supergrecko/dsa/sorting/BucketSort.scala)

## 05. Rotfaste trestrukturer

### üìöTr√¶r

Tr√¶r er en betegnelse for grafer uten sykluser hvor antall kanter er antall noder minus 1, $|E| = |V| - 1$. Et bin√¶rtre er et tre hvor hver node har 0-2 barn. Et komplett bin√¶rtre er et bin√¶rtre hvor alle l√∏vnodene ligger p√• samme h√∏yde med null barn. Et komplett bin√¶rtre har alltid $2^x$ l√∏vnoder n√•r h√∏yden er $x$. Derav ser vi ogs√• at h√∏yden av et tre med $n$ noder blir $\lg n$.

![Untitled](05/Untitled%201.png)

Det finnes flere klasser tr√¶r:

- Et ordnet tre definerer en ordning p√• barna
- Et posisjonstre har hvert barn en posisjon, og barn kan mangle.
- Et bin√¶rtre er et posisjonstre hvor hvert barn har to barneposisjoner.

CLRS definerer ikke bin√¶rtr√¶r som tr√¶r, men vi kan godt tolke dem som ordnede tr√¶r med ekstra informasjon. Dessuten kan det v√¶re hensiktsmessing √• se p√• bin√¶rtr√¶r som grafer. I noen tilfeller kan kantene i bin√¶rtreet ha piler. Det er dermed ogs√• en directed-acyclic-graph (DAG).

N√•r vi skal traversere treet har vi et par muligheter. I hvilken rekkef√∏lge skal vi h√•ndtere den n√•v√¶rende noden f√∏r vi fortsetter ned treet? Vi har tre muligheter:

- Inorder: bes√∏k alle nodene til venstre, deretter seg selv, og til slutt de p√• h√∏yre. Dette blir riktig rekkef√∏lge for bin√¶re s√∏ketr√¶r.
- Preorder: bes√∏k seg selv, deretter noder til venstre og h√∏yre
- Postorder: bes√∏k noder til venstre og h√∏yre, deretter seg selv

Det er kun $InorderTreeWalk$ som er implementert i pensum. Det er dermed √•penbart at kompleksiteten til traversering av et tre med $n$ noder er $\Theta(n)$.

[algdat/InorderWalk.scala at main ¬∑ matsjla/algdat](https://github.com/matsjla/algdat/blob/main/src/main/scala/com/supergrecko/dsa/tree/InorderWalk.scala)

### üìöBin√¶re s√∏ktr√¶r

Bin√¶re s√∏ketr√¶r er funksjonelt sett bin√¶rs√∏k i form av en datastruktur. Barnet til venstre har lavere verdi, og det til h√∏yre har en h√∏yere verdi. P√• denne m√•ten blir det veldig lett √• navigere treet.

Det er ogs√• greit √• merke seg at bin√¶re s√∏ketr√¶r i pensum ikke h√•ndterer duplikater.

![Untitled](05/Untitled%202.png)

Relevant funksjonalitet p√• et BinarySearchTree fra pensum er:

- $TreeSearch$ - s√∏k for en verdi i treet p√• kj√∏retid $O(\lg n)$, eller $O(1)$ hvis rot er tom eller s√∏keverdi.
- $TreeInsert$ - sett inn et nytt tall i treet i $O(\lg n)$ tid, eller $O(1)$ hvis treet er tomt.
- $TreeMinimum$ - finn minimumverdien treet i $O(\lg n)$ tid, eller $O(1)$ hvis treet er tomt.
- $TreeMaximum$ - finn maksverdien i treet i $O(\lg n)$ tid, eller $O(1)$ hvis treet er tomt.
- $TreeSuccessor$ - finn noden med minste verdi som er st√∏rre en den gitte noden i $O(\lg n)$ tid.
- $TreeDelete$ - slett et element fra treet i $O(\lg n)$ tid.

[algdat/BinarySearchTree.scala at main ¬∑ matsjla/algdat](https://github.com/matsjla/algdat/blob/main/src/main/scala/com/supergrecko/dsa/tree/BinarySearchTree.scala)

### üìöHauger

En haug er en datastruktur. I bunn og grunn er en haug en liste eller et array, men vi forestiller oss at haugen er et tre.

Med denne tolkningen kan vi f√• tilgang til barna eller foreldrene til en node. Hauger er automatisk s√• balanserte som mulig.

Vi ser ofte p√• $MaxHeap$ eller $MinHeap$ versjoner av hauger. Dette gir oss en gylden mulighet til √• implementere prioritetsk√∏er.

Hauger trenger ikke √• v√¶re sorterte, og siden det finnes eksponensielt mange lovlige rekkef√∏lger, gjelder ikke sorteringsgrensen.

Relevant funksjonalitet p√• en Heap fra pensum er:

- $HeapLeft$: finn posisjonen barnet til venstre til en node i konstant tid $O(1)$.
- $HeapRight$: finn posisjonen barnet til h√∏yre til en node i konstant tid $O(1)$.
- $HeapParent$: finn posisjonen foreldrenoden til en node i konstant tid $O(1)$.
- $MaxHeapify$: omplasserer en gitt node og barna til noden slik at den f√∏lger max-heap egenskapen i $O(\lg n)$ tid, men konstant $O(1)$ i beste tilfelle.
- $BuildMaxHeap$: bygg et MaxHeap av et array i line√¶r tid $O(n)$.

![Untitled](05/Untitled%203.png)

![Untitled](05/Untitled%204.png)

I tilfellet hvor haugen brukes som en prioritetsk√∏ √∏nsker vi oss f√∏lgende funksjonalitet (som ogs√• er pensum):

- $HeapMax$: finn den st√∏rste verdien i haugen i konstant tid $O(1)$.
- $HeapExtractMax$: finn og fjern det st√∏rste elementet i haugen i logaritmisk tid $O(\lg n)$.
- $HeapIncreaseKey$: √∏k verdien til en n√∏kkel i haugen i logaritmisk tid $O(\lg n)$.
- $MaxHeapInsert$: sett inn en ny verdi inn i haugen i logaritmisk tid $O(\lg n)$.

### üíªHeapSort

Til slutt kan vi bruke disse funksjonene til √• implementere en sorteringsalgoritme ved hjelp av en haug. Denne algoritmen heter $HeapSort$.

- Best case: $O(n)$
- Worst case: $O(n \lg n)$
- Average case: $O(n \lg n)$

![Untitled](05/Untitled%205.png)

---

## 06. Dynamisk programmering

### üìöOptimal delstruktur

Optimal delstruktur er en egenskap et problem kan ha, og er n√∏dvendig for at problemet kan ha en l√∏sning med dynamisk programmering. I tillegg m√• problemet ha overlappende delinstanser. N√•r et problem har overlappende delinstanser betyr det at flere oppdelinger av problemet inneholder noen av de samme delinstansene.

At et problem har optimal delstruktur betyr at det l√∏sningen til problemet kan konstrueres fra optimal l√∏sninger p√• delinstanser av problemet.

Det vil dermed v√¶re optimalt √• cache eller lagre svaret p√• delinstansene slik at man slipper √• regne ut l√∏sningen n√•r problemet oppst√•r igjen.

Man kan generelt se p√• dynamisk programmering som et ‚ÄúTime-memory tradeoff‚Äù hvor vi ofrer minne for √• lagre dell√∏sninger, men i gjengjeld kan vi spare mye tid.

![Untitled](06/Untitled%201.png)

![Untitled](06/Untitled%202.png)

![Untitled](06/Untitled%203.png)

### üíªRod Cutting Problem

Stavkapproblemet er et eksempel p√• et problem som kan l√∏ses optimal ved hjelp av dynamisk programmering.

En dum l√∏sning p√• dette problemet kan oppn√•s ved √• pr√∏ve alle kombinasjonene rekursivt, og til slutt oppn√• en kj√∏retid p√• $O(2^n)$. Dette er ikke bra nok!

![Untitled](06/Untitled%204.png)

En optimal l√∏sning vil lagre l√∏sningene p√• de forrige verdiene, og det blir dermed mulig √• sl√• opp l√∏sningen p√• tidligere delproblemer. Grafen nedenfor illustrerer hvor mange kalkulasjoner som spares p√• denne m√•ten.

![Untitled](06/Untitled%205.png)

### üìöBottom-up iterasjon vs top-down memoisering

Det finnes to hovedmetoder for √• programmere en l√∏sning som utnytter dynamisk programmering; bottom-up og top-down. Forskjellen p√• metodene handler om hvordan man designer l√∏sningen til √• utf√∏re kallene p√• delproblemene.

- Top-down: du begynner p√• hovedproblemet og finner ut hvilke dell√∏sninger som er mulige for √• komme seg dit. Her er det vanlig √• bruke rekursjon og memoisering av funksjoner.
- Botton-up: du finner ut av hvilke dell√∏sninger som kommer til √• m√•tte l√∏ses, og l√∏ser disse. Deretter jobber du deg opp mot hovedproblemet og bruker de tidligere l√∏sningene. Denne metoden kalles ofte tabulation fordi du fyller gjerne inn dell√∏sningene i en tabell.

### üíªMatrisekjedemultiplikasjon

Matrisekjedemultiplikasjon er en et problem hvor vi √∏nsker √• finne den optimale m√•ten √• gange sammen en kjede (liste) med matriser av ulik st√∏rrelse $\langle A_1, A_2, ‚Ä¶ , A_n \rangle$. =

Hvis vi har fire matriser, finnes det fem unike m√•ter √• gange disse sammen p√•, og basert p√• st√∏rrelsen p√• hver av matrisene kan antallet kalkulasjoner variere stort.

Hvis vi pr√∏ver √• gange sammen matrisene slik som vi vanligvis hadde gjort, kan det hende vi gj√∏r un√∏dvendig mange kalkulasjoner.

![Untitled](06/Untitled%206.png)

![Fire matriser har 5 unike m√•ter √• ganges sammen](06/Untitled%207.png)

Fire matriser har 5 unike m√•ter √• ganges sammen

### üíªLongest common subsequence

Gitt to sekvenser √∏nsker vi √• finne den lengste subsekvensen som er lik i begge sekvensene.

Hvis vi pr√∏ver √• l√∏se problemet ved hjelp av brute-force finner vi fort ut at vi f√•r et eksponensielt stort problem som vil v√¶re ubrukelig ved store $n$.

![Untitled](06/Untitled%208.png)

### üíªBin√¶re ryggsekkproblemet

Det bin√¶re rykksekkproblemet handler om at vi skal velge et sett med ting av forskjellig verdi og vekt slik at vi utnytter kapasiteten v√•r $W$ s√• godt som mulig.

![Untitled](06/Untitled%209.png)

---

## 07. Gr√•dighet og stabil matching

### üìöGr√•dighet

Gr√•dighet er en strategi for √• l√∏se problemer ved √• ta gr√•dige valg tidlig, og senere bevise at valget man tok var det optimale. Hittil har vi brukt forskjellige egenskaper ved problemet til √• bestemme hvilken strategi vi vil bruke for √• l√∏se det.

- Har problemet klare steg som hverken overlapper eller splitter seg? Inkrementelt design.
- Har problemet uavhengige delproblemer? Splitt og hersk.
- Har problemet overlappende problemer? Dynamisk programmering.

I tilfeller hvor dynamisk programmering m√• foreta valg, kan en gr√•dig l√∏sning ta et valg s√• lenge man kan bevise at det var det rette valget.

![Untitled](07/Untitled%201.png)

Det er ikke mulig √• bruke gr√•dighet i alle l√∏sninger. Problemet (og delproblemene) m√• oppfylle gr√•dighetsegenskapen:

- Den globale optimale l√∏sningen kan n√•s ved √• ta et optimalt lokalt valg.
- Ved √• ta et optimalt lokalt valg, m√• vi ikke ha eliminert alle mulige optimale l√∏sninger

Dessuten vil det ikke gi mening √• bruke gr√•dighet p√• et problem uten optimal delstruktur.

![Untitled](07/Untitled%202.png)

### üíªKontinuerlige ryggsekkproblemet

Det kontinuerlige ryggsegg bygger videre p√• det bin√¶re ryggsekkproblemet. Forskjeller er at du n√• har mulighet til √• velge antallet av verdien istdenfor √• enten ta eller forkaste verdien.

L√∏sningen til det kontinuerlige ryggsekkproblemet er √• ta s√• mye av det med h√∏yest verdi som mulig, fordi et annet valg vil aldri klare √• ta igjen det gr√•digste.

![Untitled](07/Untitled%203.png)

### üíªAktivitetsutvalg

Aktivitetsutvalgproblemet handler om √• velge de beste tidene p√• en timeplan av et utvalg med aktiviteter slik at timeplanen er s√• full som mulig. Dette kan for eksempel v√¶re planlegging av en forelesningsal med forskjellige forelesninger og andre aktiviteter som har lyst til √• bruke salen. Da gjelder det √• velge de beste aktivitetene slik at salen er brukt s√• mye som mulig.

![Untitled](07/Untitled%204.png)

L√∏sningen vil v√¶re √• velge aktiviteten som slutter tidligst.

### üíªHuffman

Huffman er en komprimeringsalgoritme som lager en enkoding basert p√• frekvensen til bokstavene eller tegnene i teksten. M√•let med Huffman enkodingen er √• lage en enkoding med kortest kodelengde. Dette gj√∏res ved √• bygge et Huffman-tre og deretter traversere treet for √• generere enkodingen basert p√• inputen.

![Untitled](07/Untitled%205.png)

Det optimalet treet gir bokstaver som blir brukt minst de lengste kodene.

### üíªGale-Shapley

Gale-Shapley er en stabil matchingalgoritme som matcher kvinner og menn (eller andre entiteter som studenter og skoler). En matchingalgoritme er stabil om det ikke finnes noen blokkerende par. Et blokkerende par er et par som foretrekker hverandre ovenfor parteren de har blitt tildelt.

Gale-Shapley er designet slik at matchingen er optimal for kvinnene, og pessimal for mennene.

---

## 08. Traversering av grafer

### üìöGrafrepresentasjoner

Det finnes flere m√•ter √• representere en graf i minne. I pensum er nabomatriser og nabolister i fokus. Representasjonene har noen forskjeller, og man sier ofte at nabomatriser er raskere, men de tar mer plass i minne.

Nabomatriser har en fordel siden det er mulig √• sl√• opp kanter i grafen direkte, men i hovedsak bruker vi nabolister. Dessuten er det andre faktorer som veier inn p√• hvilken representasjon vi bruker, og som nevnt finnes det mange andre representasjoner.

![Untitled](08/Untitled%201.png)

![Untitled](08/Untitled%202.png)

Nabomatriser egner seg til direkte oppslag. Nabolister egner seg til traversering. Nabolister tar ogs√• mindre plass dersom grafen har f√• kanter ‚Äì men ikke ellers!

### üìöTraversering av grafer

Traversering av en graf kan kj√∏res p√• mange forskjellige m√•ter, og pensum har tidligere v√¶rt innom traversering av et tr√¶r. M√•let med √• traversere en graf er √• unng√• √• bes√∏ke samme node to ganger, selv om grafen har sykler. Pensum nevner to enkle m√•ter √• traversere grafer p√•;

- Slett noden fra grafen
- Marker noder i grafen med farger, n√•v√¶rende node som gr√•, bes√∏kte som svart, og ubes√∏kte som hvite.

![Untitled](08/Untitled%203.png)

### üíªDepth-first search

Depth-first search er en traverseringsalgoritme som g√•r s√• dypt igjennom kantene i grafen som mulig f√∏r den backtracker og fortsetter i bredden. Dette skjer fordi depth-first search bes√∏ker alle nodene i grafen, og for hver node bes√∏ker alle nodene som er koblet til den n√•v√¶rende noden. P√• denne m√•tten vil den alltid n√• ‚Äúbunnen‚Äù f√∏r den fortsetter i bredden.

- Kompleksitet p√• $O(V + E)$ siden algoritmen bes√∏ker hver node og hver kant.

I depth-first search er det vanlig √• klassifisere nodene. Vi har f√∏lgende klasser:

- Tre-kanter: kanter som er i dybde-f√∏rst skogen, alts√• n√•r du m√∏ter en hvit node.
- Bakoverkanter: kanter til en forhjenger i skogen, alts√• n√•r du m√∏ter en gr√• node.
- Forkanter: kanter som ikke er en del av DFS-skogen.
- Krysskanter: kanter som er ikke har noen anscestor/descendant relasjon

Dessuten er det interessant √• merke seg at i dynamisk programmering utf√∏rer vi implisitt DFS p√• delproblemene.

![Untitled](08/Untitled%204.png)

### üíªBreadth-first search

Breadth-first search er en traverseringsalgoritme som g√•r s√• bredt igjennom kantene i grafen som mulig f√∏r den forsetter i dybden. Den kan ogs√• brukes til √• finne korteste vei fra en node til alle. Breadth-first search bruker en k√∏ over noder den √∏nsker √• bes√∏ke, og kan p√• denne m√•ten sikre seg at den s√∏ker i bredden f√∏rst.

- Kompleksitet p√• $O(V + E)$ siden algoritmen bes√∏ker hver node og hver kant.

### üíªTopological sort

Topological sort er en traverseingsmetode p√• en DAG som gir nodene en viss rekkef√∏lge; foreldre kommer f√∏r barn.

Det finnes flere m√•ter √• implementere topological sort p√•, og forelesning tar en kjapp titt p√• en versjon som fjerner ‚Äúsources‚Äù fra grafen. En DAG har minst en ‚Äúsource‚Äù, en node uten noen inn-kanter.

Typisk implementasjon av topological sort bruker en algoritme lik DFS med en Stack. Her kj√∏rer man DFS over grafen, deretter sortere rangere etter synkende finish-tid.

- Kompleksitet p√• $O(V + E)$ siden algoritmen bes√∏ker hver node og hver kant.

---

## 09. Minimale spenntr√¶r

### üìöDisjunkte mengder

Disjunkte mengder kommer til nytte n√•r vi √∏nsker √• finne spanntr√¶r i en graf. Her kommer litt nyttig terminologi.

- Delgraf: graf som best√•r av en delmengde av nodene og kantene til den opprinnelige grafen.
- Spenngraf: en dekkende delgraf, graf med samme nodesett som originalgrafen. (uvanlig begrep p√• norsk)
- Spennskog: en asyklisk spenngraf. (uvanlig begrep p√• norsk)
- Spenntr√¶r: en sammenhengende spennskog, alts√• $|E| = |V| - 1$.

Disjunkte mengder er en effektiv m√•te √• gruppere noder inn i sammenkoblede komponenter ved √• introdusere en ‚Äúparent‚Äù til hver node. Noden p√• topp peker deretter p√• seg selv.

![Untitled](09/Untitled%201.png)

I tillegg har hver node en rang som beskriver hvor h√∏yt opp i hierarkiet den ligger. I andre tilfeller kan denne komme til nytte, men i tilfellet med minimale spanntr√¶r √∏nsker vi kun √• gruppere nodene inn i komponenter. Denne rangen brukes for √• merge to komponenter (for √• bestemme hvilken node som dominerer). Relevant funksjonalitet fra pensom er:

- $MakeSet$ - Lag et sett med ett element av en node. Setter parent til seg selv, og rang til 0. (I konstant tid $O(1)$)
- $FindSet$ - Finn representanten (√∏verste parent) til mengden som inneholder den en gitt node, ved amortisert analyse i $O(\alpha(n))$ tid.
- $Union$ - Merge to sett sammen, hvor settet med h√∏yest rang p√• √∏verste parent dominerer den andre. Hvis begge er like, velges en vilk√•rlig.
- $Link$ - Lenker et sett til et annet, hvor settet med h√∏yest rang p√• √∏verste parent dominerer den andre. (Kalt av $Union$)

![Untitled](09/Untitled%202.png)

- $ConnectedComponents$ - Danne sammenkoblede komponenter av en graf. Dette kan gj√∏res i tiln√¶rmet $O(V + E)$ tid (variasjon, basert p√• amortisert kj√∏retid av $FindSet$).
- $SameComponents$ - Finn ut om to noder er i samme komponent, i tiln√¶rmet $O(1)$ tid (variasjon, basert p√• amortisert kj√∏retid av $FindSet$).

### üíªGenerisk MST

Kanter kan ha vekter som betyr at vi til tider √∏nsker minimale eller maksimale spenntr√¶r. Realistisk eksempel kan v√¶re bygging av veier mellom byer. Et minimalt spenntre er spenntreet som knytter sammen nodene med minst vekt mulig, og er et praktfult eksempel p√• en gr√•dig algoritme.

![Untitled](09/Untitled%203.png)

![Untitled](09/Untitled%204.png)

Eksempelet i forelesning bruker kantkontraksjon. Her vil hvert alternativ gi en ny, mindre delinstans. Dette passer godt til dynamisk programmering, men det er mye enklere √• l√∏se problemet p√• gr√•dig vis.

![Untitled](09/Untitled%205.png)

Pensum bygger videre p√• dette prinsippet gjennom to algoritmer som opererer p√• litt forskjellig vis:

- Kruskal: en kant med minimal vekt blant de gjenv√¶rende er trygg s√• lenge den ikke danner sykler
- Prim: bygger ett tre gradvis; en lett kant over snittet rundt treet er alltid trygg

### üíªKruskals algoritme

Kruskals algoritme fungerer ved √• splitte opp grafen i to skoger som man jobber med; et tre man bygger til et MST, og den andre som er en skog av disjunkt mengder.

![Untitled](09/Untitled%206.png)

![Untitled](09/Untitled%207.png)

### üíªPrims algoritme

Prims algoritme kan implementeres p√• forskjellige m√•ter, men pensum dekker en metode lik BFS ved hjelp av en min-prioritets-k√∏. Prioriteten er vekten p√• den letteste kanten mellom noden og treet.

![Untitled](09/Untitled%208.png)

![Untitled](09/Untitled%209.png)

## 10. Korteste vei fra √©n til alle

N√•r vi √∏nsker √• finne korteste vei fra √©n til alle, √∏nsker vi √• finne de korteste stiene fra en gitt node til alle andre noder. I tillegg vet vi at:

- En **enkel sti** er en sti uten sykler
- En **kortest sti** er alltid **enkel**
- Hvis det finnes en negativ sykel i veien, finnes det ingen **kortest sti**

  - Det finnes fortsatt en **kortest enklest sti** (utenfor syklen)
  - √Ö finne denne er et ul√∏sp NP-hardt problem.

  ![Untitled](10/Untitled%201.png)

![Untitled](10/Untitled%202.png)

### üíªDag-Shortest-Paths

Dette er en simpel algoritme for √• finne de korteste stiene til alle nodene fra en gitt node gitt at det ikke finnes noen sykler (positive eller negative) i nodene som kan n√•s fra startnoden. Dette er fordi algoritmen bruker en topologisk sortering n√•r den traverserer nodene i grafen.

![Untitled](10/Untitled%203.png)

![Untitled](10/Untitled%204.png)

### üíªBellman-Ford

Hvis vi har sykler grafen som kan n√•s fra startnoden kan dette l√∏ses p√• flere m√•ter. Vi kan holde styr p√• hvilke noder som endres ved iterasjon, eller oppdatere alle kantene til ingenting endrer seg.

Teoretisk sett, skal ingenting endre seg etter $|V - 1|$ iterasjoner, siden hver node i grafen skal ha blitt bes√∏kt s√• langt. Hvis ting endrer seg etter s√• mange iterasjoner m√• det finnes en negativ sykel.

![Untitled](10/Untitled%205.png)

![Untitled](10/Untitled%206.png)

### üíªDijkstras algoritme

Dijkstras algoritme tar utgangspunkt i at noden med lavest estimat m√• v√¶re ferdigkalkulert. Den eneste m√•ten vekten til stien kan bli bedre er om vi hadde negative kanter. Dijkstras algoritme fungerer dermed ikke hvis grafen har negative kanter.

![Untitled](10/Untitled%207.png)

![Untitled](10/Untitled%208.png)

Kj√∏retiden kan videre optimaliseres ved √• bruke fibonacci-heaps.

## 11. Korteste vei fra alle til alle

Trekantulikheten kan komme til nytte for √• forst√• hvordan den korteste veien kan finnes. Hvis det finnes en snarvei fra $i \to j$ som g√•r gjennom $k$, s√• har vi ikke enna funnet den korteste veien fra $i \to j$.

$$
\overline{AC} \leq \overline{AB} + \overline{BC}
$$

![Untitled](11/Untitled%201.png)

$Relax$ prosedyren v√•r sjekker om denne snarveien er kortere, og reparerer/gjeninnf√∏rer trekantulikheten. N√•r vi er ferdige, kan ikke $i \to k \to j$ fortsatt v√¶re en snarvei. ???

1. $\omega(k, j) + \delta(i, k) - \delta(i, j) \geq 0$
2. Hvis $i \to k$ og $k \to j$ finnes, finnes $i \to j$
3. $\delta(i, k) + \delta(k, j) \geq \delta(i, j)$

![$Relax(u, v, \omega)$](11/Untitled%202.png)

$Relax(u, v, \omega)$

### üíªJohnsons algoritme

Johnsons algoritme er en simpel m√•te √• finne korteste vei fra alle til alle i en graf. Den bruker i all hovedsak Dijkstras algoritme for √• finne korteste vei fra √©n til alle over alle nodene i grafen.

![Det er ogs√• vanlig √• returnere en forgjengermatrise $\Pi = (\pi_{ij})$](11/Untitled%203.png)

Det er ogs√• vanlig √• returnere en forgjengermatrise $\Pi = (\pi_{ij})$

Ettersom Dijkstras algoritme ikke fungerer med negative kanter, m√• vi balansere alle kantene i grafen helt til det ikke finnes negative kanter. Etter utregningen v√•r, kan vi omjustere igjen for √• finne de originale vektene.

> Beskrivelse bruker Turing-reduksjoner, forelesning 13-14 begrenser dette til Karp-reduksjoner.

![Untitled](11/Untitled%204.png)

N√•r vi skal √∏ke kantvektene, kan det virke intuitivt √• √∏ke alle kantene med like mye, men stier med mange kanter taper p√• denne √∏kningen. Vi vil heller gi hver node en tilordnet verdi $h(node)$. Vekten $\omega(v, u)$ √∏kes med differansen $h(u) - h(v)$. P√• denne m√•te vil positive og negative ledd bortsett fra det f√∏rste og det siste oppheve hverandre.

![Untitled](11/Untitled%205.png)

For √• sikre oss at vi ikke ender opp med noen negative vekter, kan vi igjen huske p√• trekantulikheten som forteller oss at $\omega(k, j) + \delta(i, k) - \delta(i, j) \geq 0$. Vi kan rett og sslett legge til en ny node.

Algoritmen kj√∏rer ved √• velge en tilfeldig startnode $s$ fra grafen, og deretter kj√∏re $BellmannFord$ for √• finne ut om grafen har noen negative sykler. Deretter definerer vi differansefunksjonen $h(v)$ for enhver node basert p√• distansene i resultatet fra $BellmanFord$. Vi lager deretter $\hat{w}(u, v)$ med den nye vektingen til hver kant, justert med differansen $h(u) - h(v)$. Deretter kj√∏rer vi $Dijkstra$ p√• hver node i grafen.

![Untitled](11/Untitled%206.png)

### üíª‚ÄùMatriseprodukt‚Äù

Denne algoritmen har egentlig ingen relasjon til matriseprodukt √• gj√∏re, bortsett fra at den ene prosedyren ligner litt.

![Untitled](11/Untitled%207.png)

I denne algoritmen innf√∏rer vi et parameter $r$ som er antall kanter vi har lov til √• bes√∏ke p√• stien fra $i \to j$. Vi antar induktivt at l√∏st det for alle avstander $r - 1$, og kan dermed finne $l^{(r)}_{ij} = min_k l^{(r - 1)}_{ij} + \omega(k, j)$.

Induktivt vil vi dermed finne den korteste stien ved dette ‚Äúpunktet‚Äù i grafen.

![Untitled](11/Untitled%208.png)

![Untitled](11/Untitled%209.png)

![Untitled](11/Untitled%2010.png)

Desverre er kj√∏retiden p√• denne algoritmen $O(n^4)$. (Kanskje derfor den heter ‚ÄúSlow‚Äù). Bedre versjon bruker repeated squaring.

Dette resulterer i en bedre kj√∏retid p√• $\Theta(n^3 \lg n)$

![Untitled](11/Untitled%2011.png)

### üìöTransitiv lukning

Gitt en graf, s√• √∏nsker vi √• finne ut om enhver node $i$ kan n√•s fra alle de andre nodene. Her kan vi igjen l√∏se induktivt ved √• bruke trekantulikheten.

Finnes det en sti $i \to j$ som f√•r g√• innom nodene ${1, ‚Ä¶, n}$, dvs. alle?

![Untitled](11/Untitled%2012.png)

![Untitled](11/Untitled%2013.png)

![Untitled](11/Untitled%2014.png)

For enhver vei fra $i \to j$, kan vi enten velge den direkte strekningen $i \to j$, eller s√• kan vi velge $i \to j \to k$.

![Untitled](11/Untitled%2015.png)

Forenklet implementasjon av algoritmen til h√∏yre kan implementeres med en enkel tabell. Man risikerer √• g√• innom samme node flere ganger, men det betyr bare at vi l√∏ste et senere delproblem tidligere.

Dessuten om det finnes stier med sykler, s√• finnes det ogs√• en uten.

![Untitled](11/Untitled%2016.png)

![Untitled](11/Untitled%2017.png)

![Untitled](11/Untitled%2018.png)

Totalt blir kj√∏retiden p√• $TransitiveClosure$ $\Theta(n^3)$. Analyse fra forelesning:

![Untitled](11/Untitled%2019.png)

### üíªFloyd-Warshall

Vi har allerede sett p√• m√•ter vi kan l√∏se alle til alle problemet med Dijkstras algoritme over hver node, men vi vil gj√∏re det mulig √• kj√∏re algoritmen flere typer grafer.

![Untitled](11/Untitled%2020.png)

Ved √• bygge videre p√• prinsippet fra transitiv lukning kan vi bruke samme m√•te p√• √• velge den korteste av de to veiene som forgjenger til en hver node. Det er akkurat dette $FloydWarshall$ gj√∏r.

![Untitled](11/Untitled%2021.png)

![Untitled](11/Untitled%2022.png)

![Untitled](11/Untitled%2023.png)

![Untitled](11/Untitled%2024.png)

![Untitled](11/Untitled%2025.png)

![Untitled](11/Untitled%2026.png)

Dette resulterer i en total kj√∏retid p√• $\Theta(n^3)$. I tillegg kan det v√¶re nyttig √• kunne printene veiene fra en node til en annen. Dette kan enkelt gj√∏res ved √• bes√∏ke forgjengermatrisen.

![Untitled](11/Untitled%2027.png)

## 12. Maksimal flyt

### üìöProblemet

Maksimal flyt er et problem p√• en rettet graf $G = (V, E)$. Et flytnett er grafen hvor flyten kan flyte igjennom, og den har f√∏lgende kriterier:

- Hver kant har en kapasitet $c(u, v) \geq 0$
- Det finnes ihvertfall en kilde, og en sluk $s, t \in V$
- For en hver $v \in V \implies s \to v \to t$
- Grafen har ingen self-loops
- $(u, v) \in E \implies (v, u) \not\in E$
- $(u, v) \not\in E \implies c(u, v) = 0$

Den resulterende flyten er en funksjon $f : V \times V \to \R$. Flyten p√• en kant kan aldri v√¶re st√∏rre en kapasiteten p√• kanten, alts√•:

- $0 \leq f(u, v) \leq c(u, v)$
- For en kant $u \neq s, t$, gjelder $\sum_v f(u, v) = \sum_v(v, u)$

Flytverdien $|f| = \sum_v f(s, v) - \sum_v f(v, s)$ er flyten som kan n√•s fra source til en gitt node.

Hvis du har flere kilder og sluker, legger man til en super-kilde og en super-sluk

![Untitled](12/Untitled%201.png)

Det minimale snittet er gitt etter √• ha kj√∏rt maksimal flyt p√• en graf. Den skaper en partisjon i flytnettet etter at sluket til grafen ikke kan n√•s lengre. Snittet i flytnettet er en partisjon $(S, T)$ av $V$. $s \in S$ og $t \in T$.

Netto-flyten beskriver flyten som mellom partisjonen.

![Untitled](12/Untitled%202.png)

$$
f(S, T) = \sum_{u \in S}\sum_{v \in S}f(u, v) - \sum_{u \in T}\sum_{v \in T}f(u, v)
$$

Kapasiteten blir $c(S, T) = \sum_{u \in S}\sum_{v \in T} c(u, v)$.

Til slutt beskriver Lemma 24.4: $f(S, T) = |f|$. Bevis for dette krever en del utregning, men er ganske rett frem. Videre Corollary 24.5: $|f| \leq c(S, T)$.

![Untitled](12/Untitled%203.png)

![Untitled](12/Untitled%204.png)

Maksimal flyt f√∏rer dermed til det minste snittet. Videre forteller helltallsteoremet (24.10) at for heltallskapsiteter $c(u, v) \in \N$ gir $FordFulkerson$ og andre flytalgoritmer basert p√• $FordFulkerson$ en heltallsflyt $|f| \in \N$.

### üìöReduksjon til flyt

Flyt kan nesten regnes som en designmetode siden vi kan redusere mange problemer til flytproblemet. Pensum ser p√• et par m√•ter √• bryte ned st√∏rre problemer til flytproblemer. Blant annet:

- Biparitt matching, gj√∏r om hver donor og mottaker til sink/source hvor kantene har en kapasitet p√• 1, deretter lag en supersource/supersink som vanlig. Greit √• notere seg at $EdmondsKarp$ er ikke den mest effektive m√•te √• gj√∏re dette p√•. I rekonstruksjonen representerer kanter med flyt matchingen.

  ![Untitled](12/Untitled%205.png)

- Legekontor hvor man √∏nsker minst √©n lege p√• vakt hver dag i feriene, men hver lege skal jobbe maks $c$ feriedager totalt, og maks en gang per ferie. Denne l√∏ses ved √• gj√∏re legene om til sources med kapasitet $c$, og lag en source med kapasitet 1 for hver feriedag.
- Bildesegmentering for √• separere forgrunn og bakgrunn kan l√∏ses ved √• gj√∏re hver piksel til en node, og kapasiteten fra kilde tilsvarende ‚Äúforgrunnsaktighet‚Äù og kapasitet til sluk ‚Äúbakgrunnsaktighet‚Äù. Kapaisteten mellom noder tilsvarer hvor like de er. Her vil det minimale snittet skille forgrunn fra bakgrunn.
- Veisperring kan l√∏ses ved √• gj√∏re √•stedet til source, og hvert sted man kan sette opp sperringer til sinks med kapasitet basert p√• hvor fort forbryterne kan bevege seg.

### üíªBiparitt matching

Designmetoden som brukes for biparitt matching baserer seg p√• √• gjentatte ganger konstruere et nytt, enklere problem, som gir en forbedring av den opprinnelige instansen.

Vi √∏ker matchingen ved √• finne stier fra venstre til h√∏yre (i en liggende graf). Kanter som brukes i matchingen blir snudd baklengs slik at det er mulig √• endre p√• de hvis det viser seg at det er mer optimalt senere.

Hvis vi for eksempel finner ut at en donor kan kun matches med f√∏rste resipient (men vi har allerede gitt f√∏rste resipent en donor), kan vi bruke dette til √• oppheve matchingen.

![Untitled](12/Untitled%206.png)

### üìöIdeer

Det samme gjelder for flyt. Vi har kanskje f√•tt frem √©n enhet, men hvis vi har en for√∏kende sti (sti som gir bedre resultat en n√•v√¶rende), har vi lyst til √• sende tilbake enheten. Den originale enheten (eller flyten) sendes tilbake hvor den kom fra, og s√• finner vi en ny vei frem. Da f√•r vi igjen en for√∏kende sti, og flyter fremover, og opphever den bakover til flyten n√•r der den kom fra.

![Untitled](12/Untitled%207.png)

Et restnett (residual network) er et nettverk likt flytnettet, bortsett fra at kantene mellom nodene er fremoverkanter ved ledig kapasitet, og bakoverkanter ved flyt.

En for√∏kende sti (augmenting path) er en sti fra kilde til sluk i restnettet. Langs fremoverkanter kan flyten √∏kes, og langs bakoverkanter kan flyten omdirigeres. Det er alts√• en sti hvor den totale flyten kan √∏kes.

Det er dermed nyttig √• finne ut hva potensialet (eller restkapasiteten) mellom to noder er. Hvor mye kan vi √∏ke flyten fra $u$ til $v$?

![Untitled](12/Untitled%208.png)

### üíªFord-Fulkerson

$FordFulkerson$ er en generell metode for √• finne den maksimale flyten gjennom et nettverk. En versjon av $FordFulkerson$ er $EdmondKarp$ som bruker en BFS. Ford-Fulkersom fungerer ved √•:

1. Finn for√∏kende stier s√• lenge det g√•r
2. Deretter er flyten s√• stor den kan bli.

Dette gj√∏res som regel ved √• finne den for√∏kende stien, deretter finner man flaskehalsen i stien, og oppdaterer flyt langs stien med denne verdien.

![Untitled](12/Untitled%209.png)

En mer konkret beskrivelse av algoritmen finnes i pensum:

![Untitled](12/Untitled%2010.png)

Alternativt kan man flette inn BFS ved √• finne flaskehalser underveis. Hold deretter styr p√• hvor mye flyt vi f√•r frem til hver node, og traverser kun noder vi ikke har n√•dd frem til enda.

Denne implementasjonen st√•r ikke i boka, og det er ikke krav √• kunne denne i detalj.

![Untitled](12/Untitled%2011.png)

![Untitled](12/Untitled%2012.png)

![Untitled](12/Untitled%2013.png)

N√•r vi hr kj√∏rt Ford-Fulkerson kan vi finne det minimale snittet direkte fra resultatet. Forelesningsnotatet inneholder mer om bevis p√• korrekthet.

Kj√∏retiden p√• Ford-Fulkerson kan g√• ille hvis vi ikke bruker BFS. En graf som ser slik ut, vil kunne sende flyten fra og tilbake p√• noden med 1 kapasitet flere millioner ganger.

Med irrasjonale kapasiteter kan vi ikke garantere at Ford-Fulkerson terminerer,

Hvis vi bruker BFS, vil f√•r vi en kj√∏retid p√• $O(VE^2)$ for √• finne for√∏kende stier.

Pensum har en lengre forklaring p√• hvorfor vi f√•r $O(VE)$ i indre l√∏kke.

![Untitled](12/Untitled%2014.png)

[https://folk.idi.ntnu.no/mlh/algkon/flow.pdf](https://folk.idi.ntnu.no/mlh/algkon/flow.pdf)

## 13. NP-kompletthet

### üìöReduksjon

I denne forelesningen betegner reduksjoner Karp-reduksjoner, eller ‚Äúmany-one‚Äù reduksjoner som tar polynomisk lang tid.

Vi √∏nsker √• beskrive problemer sin vanskelighetsgrad. Det er teoretisk mulig √• finne absolutt vanskeliggrad av et problem, men det er lite nyttig, s√• vi vil heller sammenligne og kategorisere problemer.

Et eksempel p√• en Karp-reduksjon fra forelesning:

> Gitt en kiste $A$, og en kiste $B$ hvor n√∏kkelen til $A$ ligger inni $B$. Ved reduksjon er det dermed √•penbart at det ikke kan v√¶re vanskeligere √• √•pne $A$ enn det er √• √•pne $B$.

Hva ‚Äúminst like vanskelig‚Äù betyr kommer an p√• reduksjonene, men i v√•rt tilfelle bruker vi polynomiske reduksjoner som betyr at problemene kan l√∏ses i polynomisk tid.

Hvis vi √∏nsker √• l√∏se problemet ‚Äúhar en graf en kort sti mellom node $u$ og $v$‚Äù kan vi redusere dette ned til √• finne korteste sti mellom nodene. Det er dermed √•penbart at √• finne ut om det finnes en kort sti, er minst like vanskelig som √• finne korteste sti.

![Untitled](13/Untitled%201.png)

![Untitled](13/Untitled%202.png)

![Untitled](13/Untitled%203.png)

Hvis vi har et problem $\alpha$ som vi kan polynomisk redusere til et annet problem $\beta$ som ogs√• er polynomisk, er det √•penbart at vi kan bruke l√∏sningen $B(\beta)$ for √• lage en l√∏sning $A(\alpha)$.

![Untitled](13/Untitled%204.png)

Vi kan redusere Hamilton-sykel problemet ned til Long-Path problemet i polynomisk tid, s√• det betyr at $HamCycle$ kan umulig v√¶re vanskeligere enn $LongPath$, og $LongPath$ er minst like vanskelig som $HamCycle$.

P√• denne m√•ten kan vi ogs√• redusere perfekt matching til $HamCycle$, og det betyr at $Match$ kan umulig v√¶re vanskeligere enn $HamCycle$, og dermed ikke vanskeligere enn $LongPath$. Det eneste problemet er at vi ikke vet hvordan vi kan finne en l√∏sning p√• $LongPath$.

### üìöVerifikasjon

Beslutningsproblemer er problemer vi kan verifiserer et sertifikat for et ja-svar. Hvis vi lurer p√• om $X$ finnes, kan vi bruke verifikasjonsfunksjonen til √• sjekke enhver mulig $X$ for en instans. Verifikasjonsmetoden i dette tilfellet kj√∏rer i polynomisk tid slik at det er mulig √• verifisere det. Hvis svaret er ‚Äúnei‚Äù, s√• har vi ingen sertifkat for problemet, ingenting √• verifisere.

![Untitled](13/Untitled%205.png)

Et beslutningsproblem stiller rett og slett sp√∏rsm√•let ‚Äúfinnes det et sertifikat for at svaret er ja?‚Äù for et slikt sertifikat skal finnes hvis, og bare hvis svaret er ‚Äúja‚Äù.

- Problemklassen **P** er problemer som kan l√∏ses i polynomisk tid.
- Problemklassen **NP** har ja-svar sertifikater som kan sjekkes i polynomisk tid. Problemer i **NP** som ogs√• er i **P** er selvf√∏lgelig l√∏sbare i polynomisk tid.
- Problemklassen **NPC** kan vi ikke l√∏se, men vi kan verifisere de i polynomisk tid. NP-komplette problemer er subsettet av **NP** som alle andre **NP** problemer kan reduseres til i polynomisk tid.
- Problemklassen **NPH** kan ikke l√∏ses eller verifiseres, men de som ogs√• er i **NPC** kan selvf√∏lgelig sjekkes i polynomisk tid. Dette er problemer som er minst like vanskelige som **NPC**, men de trenger ikke √• v√¶re i **NP**, og trenger ikke √• v√¶re beslutningsproblemer.
- Problemklassen **co-NP** har nei-svar som kan sjekkes i polynomisk tid.

Dette betyr at hvis man finner et bevis for √• l√∏se et NP-komplett problem, kan man l√∏se alle andre NP problemer.

Det er ogs√• praktisk √• se p√• beslutningsproblemene som mengder av instanser (bitstrenger) der svaret er ja. En slik mengde er et formelt spr√•k.

![Untitled](13/Untitled%206.png)

[What are the differences between NP, NP-Complete and NP-Hard?](https://stackoverflow.com/a/19510170/14996499)

### üìöKompletthet

Vi har et univers av problemer og vi kan sammenligne dem. Det gir faktisk opphav til et sett med maksimalt vanskelige problemer. Disse kalles komplette for klassen NP under polynomiske reduksjoner.

Hvis vi har en mengde med kister, og en ekstra kiste $K$ som har n√∏kkelen til alle de andre, s√• kan vi redusere √•pne alle kistene til √•pne $K$. Kompletthet er en universaln√∏kkel for alle problemene. Finner vi n√∏kkelen her, kan vi l√∏se alle de andre problemene.

Slik ser vi for oss at hierarkiet til problemene er, selv om det ikke er helt mulig √• vite. Det finnes ogs√• mange andre klasser med problemer, og det er mange mulige scenarier. Problemet er at ingen vet hva som stemmer.

Se slides for bedre beskrivelse

![Untitled](13/Untitled%207.png)

Hvis det har seg slik at $P = NP$, kan vi ikke bare svare p√• om det finnes et sertifikat, men vi kan rekonstruere sertifikatet. Med andre ord, kan vi l√∏se beslutningsproblemer, s√• kan vi l√∏se s√∏keproblemer ogs√•, og finne gyldig output.

### üíªOppfyllbarhet

Oppfyllbarhetsproblemet var det f√∏rste problemet som ble bevist til √• v√¶re **NPC**. Det g√•r ut p√• √• finne ut om det er mulig at en logisk krets sine inputs kan bestemme om det er mulig at output er 1.

![Untitled](13/Untitled%208.png)

![Untitled](13/Untitled%209.png)

![Untitled](13/Untitled%2010.png)
