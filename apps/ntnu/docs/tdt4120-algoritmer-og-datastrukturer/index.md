---
sidebar_position: 1
slug: /tdt4120-algoritmer-og-datastrukturer
---

# TDT4120: Algoritmer og datastrukturer

Tatt hÃ¸sten 2022. Nedenfor er notater fra alle forelesningene bortsett fra forelesning 14.

## 01. Problemer og algoritmer

### ğŸ“šForkunnskaper og pseudokode

Forkunnskaper til emner er logaritmer, spesielt logaritmisk notasjon og regneregler for logaritmer slik at det er enklere Ã¥ se at teoremer som masterteoremet gÃ¥r opp. Det er i tillegg nÃ¸dvendig Ã¥ kjenne til modulo operatoren som er resten av en brÃ¸k.

LÃ¦reboka er CLRS ([https://en.wikipedia.org/wiki/Introduction_to_Algorithms](https://en.wikipedia.org/wiki/Introduction_to_Algorithms)) 4. edition. Boka har gÃ¥tt igjennom betydelige endringer siden 3. edition og anbefales dermed ikke. Pseudokoden i boka antar i de fleste tilfeller at indeksing av lister begynner pÃ¥ 1.

### ğŸ“šProblemlÃ¸sing og dekomponering

Det finnes mange problemer som datamaskiner kan lÃ¸se, men ikke alle lÃ¸sninger til problemene fungerer, spesielt ikke pÃ¥ stor skala.

> Eksempel fra forelesning: hvis vi skulle brute-force matching mellom donor og mottaker sÃ¥ hadde vi kun matchet i overkant av hundre personer pÃ¥ 10000000â€¦ ganger universets levetid. Dette er estimert til Ã¥ ta en halvtime for alle menneskene pÃ¥ kloden med Chandran-Hochbaum algoritmen.

Det er derfor viktig at vi bruker en mer optimal lÃ¸sning for Ã¥ lÃ¸se problemet. Hvis vi bruker en ordentlig algoritme istedenfor Ã¥ brute-force problemet kan vi nÃ¥ realistiske kjÃ¸retider.

Et problem kan beskrives som en general relasjon mellom input og output. Begrepet for en enkel, konkret input er en instans. Siden algoritmer ofte jobber pÃ¥ en eller annen type datastruktur som en liste eller et tre kan det vÃ¦re nyttig Ã¥ dekomponere problemet i mindre problemer (instanser).

PÃ¥ denne mÃ¥ten kan vi fÃ¸lge en metode lik matematisk induksjon for Ã¥ skape lÃ¸sninger for problemene. Vi bryter problemet ned til et â€œbase caseâ€ som vi kan bevise at stemmer. Deretter bygger vi opp lÃ¸sningen ved hjelp av induksjon.

![Untitled ](01/Untitled%201.png)

![Untitled ](01/Untitled%202.png)

![Untitled ](01/Untitled%203.png)

For Ã¥ gjÃ¸re det enklere Ã¥ bryte opp problemet kan vi bruke fÃ¸lgende tabell for Ã¥ fÃ¥ en fin mental oversikt over problemet vi Ã¸nsker Ã¥ lÃ¸se. FÃ¸rst fyller vi ut â€œInstansâ€ og â€œLÃ¸sningâ€ som tilsvarer input og output til problemet. Deretter jobber vi oss nedover pÃ¥ vestre siden og til slutt gÃ¥r vi opp pÃ¥ hÃ¸yre side og lÃ¸ser resten av problemet. Nedenfor er eksempel for lÃ¸sing av insertion-sort.

![Untitled ](01/Untitled%204.png)

![Untitled ](01/Untitled%205.png)

I kode har vi to mÃ¥ter Ã¥ lÃ¸se slike rekursive induksjonsproblemer; lÃ¸kker og rekursjon. En lÃ¸kkeinvariant er et delsteg i lÃ¸sningen for en iterasjon av lÃ¸sningen til problemet. Vi beviser at lÃ¸sningen vÃ¥r er rett for grunntilfellet, og gjennom hvert steg i lÃ¸kkeinvarianten, beviser at dellÃ¸sningen er rett bÃ¥de fÃ¸r og etter. PÃ¥ denne mÃ¥ten kan vi induktivt bevise at sluttlÃ¸sningen vÃ¥r er riktig. Om dette gjÃ¸res ved en faktisk lÃ¸kke eller rekursjon har ikke noe Ã¥ si. Som alltid, mÃ¥ vi bevise at lÃ¸kka eller rekursjonen stopper.

### ğŸ’»Insertion Sort

Insertion Sort er en sorteringsalgoritme som fungerer ved Ã¥ ta hvert element i lista, og flytte det mot venstre helt til det ligger i riktig posisjon i den sorterte lista. Insertion Sort er en stabil sorteringsalgoritme som betyr at den beholder rekkefÃ¸lgen pÃ¥ like elementer.

- Best case: $O(n)$ hvis lista er sortert fra fÃ¸r av
- Worst case: $O(n^2)$ hvis lista er sortert fra hÃ¸yt til lavt (revers)
- Average case: $O(n^2)$

[algdat/InsertionSort.scala at main Â· matsjla/algdat](https://github.com/matsjla/algdat/blob/main/src/main/scala/com/supergrecko/dsa/sorting/InsertionSort.scala)

### ğŸ“šAsymptotisk notasjon

Asymptotisk notasjon er notasjon som ofte brukes for Ã¥ vise hvor ressurseffektiv en algoritme eller lÃ¸sning er. Man ser ofte pÃ¥ kjÃ¸retid, men man kan ogsÃ¥ ha asymptotisk notasjon av minnebruk, instruksjonssykluser og liknende data. Nedenfor er skalaen pÃ¥ de forskjellige formene.

![Untitled ](01/Untitled%206.png)

NÃ¥r vi bruker asymptotisk notasjon er vi kun interessert i en veldig grov stÃ¸rrelsesorden. Dermed dropper vi konstanter og lavere ordens ledd.

$$
\Theta(n^2 + n + 10) = \Theta(n^2)
$$

Reell eller nÃ¸yaktiv kjÃ¸retid kan vÃ¦re varierende eller udefinert. Vi Ã¸nsker dermed Ã¥ finne en form som likner kurven. Vi bruker tre forskjellige mÃ¥linger i asymptotisk notasjon. Det er viktig Ã¥ vite at det finnes ingen direkte sammenheng mellom $O(n)$, $\Theta(n)$, og $\Omega(n)$ og best/worst/average case kompleksitet for en algoritme. Asymptotisk notasjon er bare en mÃ¥te Ã¥ definere best/worst/average case.

- $O(n)$ eller big-o definerer den Ã¸vre grensa. Dette betyr at funksjonen kan vÃ¦re opptil $n$ i kompleksitet, men ogsÃ¥ veldig mye mindre.
- $\Theta(n)$ eller stor theta definerer â€œtightest boundâ€. Dette betyr at funksjonen vil vÃ¦re tilnÃ¦rmet $n$. Det impliserer ogsÃ¥ $O(n) = \Omega(n) \implies \Theta(n)$
- $\Omega(n)$ eller stor omega definerer den nedre grensa. Dette betyr at funksjonen har ihvertfall $n$ i kompleksitet, men det er ingen Ã¸vre grense som betyr at den kan vÃ¦re uendelig kompleks.

I tillegg har vi $o(n)$ og $\omega(n)$ som er Ã¸vre, men ikke lik, og nedre, men ikke lik grenser.

NÃ¥r en skal regne pÃ¥ asymptotisk notasjon hjelper det Ã¥ tenke pÃ¥ at notasjonen $O(n)$ definerer mengden av alle funksjonene med kompleksitet $n$ eller mindre. Det resulterer ofte iat $\Omega(n)$ dominerer et uttrykk. Dessuten er $f(n) = O(n)$ notasjon misbruk da $O(n)$ er en mengde.

![Untitled ](01/Untitled%207.png)

---

## 02. Datastrukturer

### ğŸ’»Stack

En Stack er en lineÃ¦r last-in-first-out (LIFO) datastruktur. En stack fungerer som en stabel hvor elementene som legges pÃ¥ fÃ¸rst, kommer ut sist. Relevant funksjonalitet pÃ¥ en Stack i pensum er:

- $StackEmpty$ - er stacken tom?
- $StackPush$ - legg til et element pÃ¥ toppen av stacken i konstant tid $O(1)$
- $StackPop$ - fjern et element fra toppen av stacken i konstant tid $O(1)$

[algdat/Stack.scala at main Â· matsjla/algdat](https://github.com/matsjla/algdat/blob/main/src/main/scala/com/supergrecko/dsa/linear/Stack.scala)

### ğŸ’»Queue

En Queue er en lineÃ¦r first-in-first-out (FIFO) datastrukturo. En Queue fungerer pÃ¥ lik mÃ¥te som en Stack, bortsett fra at elementene som ble lagt pÃ¥ fÃ¸rst, kommer ut fÃ¸rst. Relevant funksjonalitet pÃ¥ en Queue fra pensum er:

- $QueueEnqueue$ - legg til et element i kÃ¸en i konstant tid $O(1)$
- $QueueDequeue$ - ta det fremste elementet i kÃ¸en i konstant tid $O(1)$
- $QueueSize$ - stÃ¸rrelsen pÃ¥ kÃ¸en, som regel kun relevant hvis kÃ¸en ikke backes av en resizable container.

Implementasjonen i pseudokode i pensum tar ikke hensyn til overflow eller underflow av kÃ¸en. Implementasjon i Scala tar hensyn til dette.

[algdat/Queue.scala at main Â· matsjla/algdat](https://github.com/matsjla/algdat/blob/main/src/main/scala/com/supergrecko/dsa/linear/Queue.scala)

### ğŸ’»LinkedList

En lenket liste (i pensum, doubly-linked-list) er en liste som bestÃ¥r av noder som er allokert tilfeldige steder i minnet som er koblet sammen via pekere. Det tar dermined lineÃ¦r tid Ã¥ slÃ¥ opp pÃ¥ en gitt posisjon, men det er konstant tid for Ã¥ sette inn eller slette elementer. Relevant funksjonalitet pÃ¥ en LinkedList fra pensum er:

- $ListSearch$ - sÃ¸k igjennom en liste. Beste tilfelle pÃ¥ $O(1)$ hvis fÃ¸rste element er sÃ¸keelementet, ellers er det gjennomsnittslig og verste tilfelle pÃ¥ $O(n)$.
- $ListPrepend$ - legg til en node i frontend av lista i konstant tid $O(1)$.
- $ListInsert$ - legg til en ny node $x$ foran en eksisterende node $y$ i konstant tid $O(1)$. Denne funksjonen trenger ikke et liste objekt da den kun operer pÃ¥ to noder.
- $ListDelete$ - slett en node $x$ fra lista i konstant tid $O(1)$

[algdat/LinkedList.scala at main Â· matsjla/algdat](https://github.com/matsjla/algdat/blob/main/src/main/scala/com/supergrecko/dsa/linear/LinkedList.scala)

### ğŸ’»HashTable

En hashtabell er en relasjon eller mapping mellom en nÃ¸kkel pÃ¥ en verdi. En hashtabell implementeres som regel ved hjelp av en lineÃ¦r datastruktur av LinkedList-er. NÃ¸kkelen gÃ¥r igjennom en hash-funksjon som bestemmer hvilken bÃ¸tte verdien skal havne i. En god hash-funksjon vil ha sÃ¥ fÃ¥ kollisjoner som mulig som gjÃ¸r innsetting til, og henting fra tabellen ta konstant tid.

I tilfelle hvor det er kollisjoner, lagres ogsÃ¥ noe ekstra data (gjerne nÃ¸kkelen eller annen data som kan gjenkjenne nÃ¸kkelen) for Ã¥ lÃ¸se kollisjoner. Dette resulterer i veldig simple algoritmer for Ã¥ bruke hashtabellen. Relevant funksjonalitet pÃ¥ et HashTable fra pensum er:

- $ChainedHashSearch$ - finn verdien til en nÃ¸kkel i hashtabellen. KjÃ¸rer i beste og gjennomsnittslig tilfelle i konstant tid $O(1)$, men siden det kan vÃ¦re kollisjoner er verste tilfelle $O(n)$.
- $ChainedHashInsert$ - sett inn gitt nÃ¸kkel og verdi i lista i konstant tid $O(1)$.
- $ChainedHashDelete$ - fjern verdien til en nÃ¸kkel i hashtabellen. KjÃ¸rer i beste og gjennomsnittslig tilfelle i konstant tid $O(1)$, men siden det kan vÃ¦re kollisjoner er verste tilfelle $O(n)$.

[algdat/HashTable.scala at main Â· matsjla/algdat](https://github.com/matsjla/algdat/blob/main/src/main/scala/com/supergrecko/dsa/linear/HashTable.scala)

### ğŸ’»DynamicTable

En dynamisk tabell er en liste eller et array som har evnen til Ã¥ gro eller minke i stÃ¸rrelse basert pÃ¥ antall elementer i lista. Det betyr at lista kan bli (i teorien) uendelig stor siden vi kan gro lista nÃ¥r den nÃ¦rmer seg full. Det er vanlig Ã¥ bruke en stÃ¸rre faktor enn $n+1$ for Ã¥ gro tabellen for Ã¥ unngÃ¥ mange allokasjoner. Relevant funskjonalitet pÃ¥ et DynamicTable fra pensum er:

- $TableInsert$ - legg til et nytt element i den dynamiske lista. KjÃ¸rer beste og gjennomsnittslig tilfelle i konstant tid $O(1)$, men siden lista mÃ¥ gro innimellom er verste tilfelle $O(n)$.

[algdat/DynamicTable.scala at main Â· matsjla/algdat](https://github.com/matsjla/algdat/blob/main/src/main/scala/com/supergrecko/dsa/linear/DynamicTable.scala)

### ğŸ“šAmortisert arbeid

KjÃ¸retiden for Ã©n enkelt operasjon er ikke alltid like informativt. I en dynamisk tabell kan en insert operasjon fÃ¥ lista til Ã¥ gro og dermed bli $O(n)$, men siden vi fÃ¥r en stÃ¸rre grofaktor vil dette som regel ikke skje. Amortisert arbeid er aggregert analyse av et problem. Vi finner totalt arbeid og deler pÃ¥ antall operasjoner.

Average-case kommer dermed av forventet kjÃ¸retid for en algoritme, selv om den kan i noen tilfeller vÃ¦re mye tregere (som i $DynamicTableInsert$). Amortisert arbeid er snitt over operasjoner.

---

## 03. Splitt og hersk

### ğŸ“šSplitt og hersk

Splitt og hersk er et paradigme for Ã¥ bryte ned et kompleks problem inn i mindre underproblemer og lÃ¸se underproblemene for seg selv. Deretter setter vi sammen dellÃ¸sningene for Ã¥ lage en induktivt riktig lÃ¸sning.

### ğŸ“šRekurrenser

Rekurrenser er rekursive likninger, gjerne pÃ¥ formen $f(n) = f(n - 1) + c$. Problemet med slike definisjoner er at de er rekursive som betyr at vi kan prÃ¸ve Ã¥ lÃ¸se de, men utfordringen er Ã¥ komme seg til bunns og deretter nÃ¸ste seg opp igjen.

![Untitled](03/Untitled%201.png)

Rekurrenser har som regel et grunntilfelle som gjÃ¸r det enklere Ã¥ nÃ¥ bunnen.

$$
f(n) = \begin{cases}n = 1 & 1\\n \neq 1 & f(n - 1) + 1\end{cases}
$$

Rekurrenser er nyttige fordi de kan beskrive kompleksiteten eller kjÃ¸retiden til rekursive algoritmer. Vi kan lÃ¸se rekurrenser med:

- Iterasjonsmetoden: gjenntatt ekspandering av rekurrensen til vi fÃ¥r en sum vi kan regne ut
- RekursjonstrÃ¦r: ekspander rekurrensen og lag et tre man kan summere pÃ¥. TODO
- Masterteoremet: en enkel mÃ¥te Ã¥ lÃ¸se rekurrenser av formen $T(n) = a \cdot T(\frac{n}{b}) + f(n)$. Mer om masterteoremet kommer senere i notatet.

Til slutt verifiserer vi lÃ¸sningen vÃ¥r med induksjonsbevis.

### ğŸ’»Binary Search

BinÃ¦rsÃ¸k er en divide & conquer sÃ¸kealgoritme som opererer pÃ¥ en sortert liste. Den velger elementet i midten av lista og sammenligner det med sÃ¸keelementet, og kaller seg selv rekursivt pÃ¥ enten hÃ¸yre eller venstreside av lista. Grunntilfellet er en tom liste hvor vi kan konkludere med at elementet ikke eksisterer i lista.

- Best case: $O(1)$ hvis sÃ¸keelementet er i midten av lista
- Worst case: $O(\lg n)$ siden rekursjonstreet har hÃ¸yde $\lg n$
- Average case: $O(\lg n)$

[algdat/BinarySearch.scala at main Â· matsjla/algdat](https://github.com/matsjla/algdat/blob/main/src/main/scala/com/supergrecko/dsa/search/BinarySearch.scala)

### ğŸ’»Merge Sort

Merge Sort er en divide & conquer sorteringsalgoritme. Den drives av subprosedyren $Merge$ som tar inn en liste, splitter det i to og sorterer sublistene. Siden merge kalles etter de rekursive kallene til $MergeSort$ vil det via induksjon bevises at halvsidene av lista er sortert fÃ¸r de merges. Grunntilfellet er en tom liste hvor det ikke finnes mer arbeid Ã¥ gjÃ¸re.

- Best case: $O(n \lg n)$
- Worst case: $O(n \lg n)$
- Average case: $O(n \lg n)$

Kompleksiteten blir $O(n \lg n)$ fordi $Merge$ har en kompleksistet pÃ¥ $n$, ganget med hÃ¸yden pÃ¥ rekursjonstreet som er $\lg n$.

[algdat/MergeSort.scala at main Â· matsjla/algdat](https://github.com/matsjla/algdat/blob/main/src/main/scala/com/supergrecko/dsa/sorting/MergeSort.scala)

### ğŸ’»Quick Sort

Quick Sort er en divide & conquer sorteringsalgoritme. Den drives av subprosedyren $Partition$ som velger et pivotelement som plasseres pÃ¥ riktig plass i den sorterte lista, samtidig som den sÃ¸rger for at alle elementer til venstre er lavere, og alle til hÃ¸yre er hÃ¸yere. Siden partition kalles fÃ¸r det rekursive kallet til $QuickSort$ partisjoneres alle breakpoints i lista og dermed kan vi ved induksjon bevise at lista er sortert. Grunntilfellet er tom liste hvor det ikke finnes mer arbeid Ã¥ gjÃ¸re.

- Best case: $O(n \lg n)$
- Worst case: $O(n^2)$ hvis lista var sortert fra fÃ¸r av og pivot velges som siste element
- Average case: $O(n \lg n)$

For Ã¥ unngÃ¥ tilfellet hvor dÃ¥rlig valg av pivot oppstÃ¥r finnes en alternativ mÃ¥te Ã¥ velge pivot pÃ¥; randomisert Quick Sort. Her vil verste tilfelle bli $O(n \lg n)$ nÃ¥r $n \to \infty$ ettersom det blir tilnÃ¦rmet null sjanse for at verste tilfellet oppstÃ¥r.

[algdat/QuickSort.scala at main Â· matsjla/algdat](https://github.com/matsjla/algdat/blob/main/src/main/scala/com/supergrecko/dsa/sorting/QuickSort.scala)

[algdat/RandomizedQuickSort.scala at main Â· matsjla/algdat](https://github.com/matsjla/algdat/blob/main/src/main/scala/com/supergrecko/dsa/sorting/RandomizedQuickSort.scala)

### ğŸ“šMasterteoremet

Masterteoremet er en veldig enkel metode for Ã¥ lÃ¸se rekurrenser pÃ¥ formen $T(n) = a \cdot T(\frac{n}{b}) + f(n)$. Her gjelder det bare Ã¥ vite at $a^{\lg n} = n^{\lg a}$. Ved Ã¥ se pÃ¥ hvordan logaritmen skisses pÃ¥ grafen er det enkelt Ã¥ se at disse to er like.

![Untitled](03/Untitled%202.png)

Vi kan bruke iterasjonsmetoden for Ã¥ finne et mÃ¸nster i den rekursive funksjonen. Vi fÃ¥r dermed en lÃ¸sning pÃ¥ formen $a^?T(\frac{n}{b^?})$. Her setter vi inn $? = \lg_b n$.

![Untitled](03/Untitled%203.png)

$$
a^{\lg_b n} \cdot T(\frac{n}{b^{\lg_b n}}) \\
a^{\lg_b n} \cdot T(\frac{n}{n}) \\
a^{\lg_b n} \cdot 1 \\
a^{\lg_b n} \\
n^{\lg_b a}
$$

Til slutt sammenligner vi resultatet med den drivende funksjonen $f(n)$.

![Untitled](03/Untitled%204.png)

---

## 04. Rangering i lineÃ¦r tid

### ğŸ“šSortingsgrensen

Det finnes en nedre grense for hvor raske rangeringsalgoritmer kan vÃ¦re. Denne grensa er pÃ¥ $\Omega(n \lg n)$. Dette er synlig i divide-and-conquer algoritmer som deler opp problemet pÃ¥ en mÃ¥te som gjÃ¸r det mulig Ã¥ forestille seg et rekursjonstre.

![[https://cs.stackexchange.com/questions/32311/proving-the-lower-bound-of-compares-in-comparison-based-sorting/32312#32312](https://cs.stackexchange.com/questions/32311/proving-the-lower-bound-of-compares-in-comparison-based-sorting/32312#32312)](04/Untitled%201.png)

[https://cs.stackexchange.com/questions/32311/proving-the-lower-bound-of-compares-in-comparison-based-sorting/32312#32312](https://cs.stackexchange.com/questions/32311/proving-the-lower-bound-of-compares-in-comparison-based-sorting/32312#32312)

Likevel finnes det algoritmer som $InsertionSort$ som kjÃ¸rer i $O(n)$ i beste tilfelle, men det er kun pÃ¥ grunn av hvordan algoritmen er skrevet. Dessuten kan vi utnytte sorteringsgrensen, og til og med bryte den ved Ã¥ skrive smarte algoritmer. Et eksempel pÃ¥ dette er $RandomizedSelect$ som kommer senere. Her er generell tabell for sorteringsgrensa.

|          | Best        | Average           | Worst             |
| -------- | ----------- | ----------------- | ----------------- |
| $O$      | $O(\infty)$ | $O(\infty)$       | $O(\infty)$       |
| $\Theta$ | $\Theta(?)$ | $\Theta(?)$       | $\Theta(?)$       |
| $\Omega$ | $\Omega(n)$ | $\Omega(n \lg n)$ | $\Omega(n \lg n)$ |

Det er umulig Ã¥ si noe om den generelle Ã¸vre grensa ettersom algoritmen kan ta uendelig med tid for alt vi vet. Dessuten er det ikke mulig Ã¥ gÃ¥ lavere enn $n$ fordi vi mÃ¥ garantere at hele sekvensen behandles.

### ğŸ“šReduksjonsbevis

Vi kan bevise egenskaper til enkelte problemer ved Ã¥ bruke de som del-lÃ¸sning av et annet problem. Dette kalles reduksjonsbevis. Et eksempel pÃ¥ dette er unikhetsproblemet.

![Untitled](04/Untitled%202.png)

I dette problemet blir vi fortalt at man kan i verste tilfelle finne ut om en tabell har duplikater i $\Omega(n \lg n)$ tid. Siden sortering er mesteparten av arbeidet lÃ¸sningen kan det umulig ha seg at sortering i verste tilfelle er bedre enn $\Omega(n \lg n)$.

![Untitled](04/Untitled%203.png)

> I forelesning 4 presenteres en analogi for reduksjonsbeviset med en lÃ¥st kiste. Det kan umulig vÃ¦re lettere Ã¥ Ã¥pne kista enn det er Ã¥ Ã¥pne lÃ¥sen.

### ğŸ’»Randomized Select

Randomized Select er en divide & conquer algoritme som effektivt fungerer som â€œQuickSort som BinarySearchâ€. Randomized Select finner elementet pÃ¥ $i$-ende indeks i lista. Vi bruker faktum at $Partition$ subprosedyren til $QuickSort$ plasserer pivotelementet pÃ¥ riktig plass, og sÃ¸rger for at elementene til vestre er lavere, og elementene til hÃ¸yre er hÃ¸yere. PÃ¥ denne mÃ¥ten kan vi splitte lista igjen og finne elementet pÃ¥ en gitt indeks.

- Best case: $O(n)$ av rekurrensen $2n - 1$
- Average case: $O(n)$
- Worst case: $O(n^2)$ hvis pivotelementet er alene, slik som i $QuickSort$. Lite sannsynlig til Ã¥ skje med randomisering.

I tillegg finnes en annen select algoritme som heter $Select$. Denne er mer kompleks og grundig forstÃ¥else er ikke et krav i pensum.

[algdat/RandomizedSelect.scala at main Â· matsjla/algdat](https://github.com/matsjla/algdat/blob/main/src/main/scala/com/supergrecko/dsa/selection/RandomizedSelect.scala)

### ğŸ’»Counting Sort

Counting Sort er en stabil sorteringsalgoritme som bryter sorteringsgrensen ved Ã¥ gjÃ¸re en antakelse at ingen elementer er hÃ¸yere enn en gitt verdi $k$. Deretter lager den en liste som holder styr pÃ¥ hvilken indeks ethvert element i lista skal vÃ¦re i den endelige lista.

- Best case: $\Theta(n + k)$
- Worst case: $\Theta(n + k)$
- Average case: $\Theta(n + k)$

[algdat/CountingSort.scala at main Â· matsjla/algdat](https://github.com/matsjla/algdat/blob/main/src/main/scala/com/supergrecko/dsa/sorting/CountingSort.scala)

### ğŸ’»Radix Sort

Radix Sort er en stabil sorteringsalgoritme som bryter sorteringsgrensen ved Ã¥ gjÃ¸re en antaklese at ingen elementer har flere enn $d$ siffer. Deretter bruker den en stabil sorteringsalgoritme som $CountingSort$ til Ã¥ sortere etter hvert siffer.

- Best case: $\Theta(d(n + k)$
- Worst case: $\Theta(d(n + k)$
- Average case: $\Theta(d(n + k)$

[algdat/RadixSort.scala at main Â· matsjla/algdat](https://github.com/matsjla/algdat/blob/main/src/main/scala/com/supergrecko/dsa/sorting/RadixSort.scala)

### ğŸ’»Bucket Sort

Bucket Sort er en sorteringsalgoritme som bryter sorteringsgrensen ved Ã¥ anta at alle verdier er flyttall mellom $[0, 1)$ pÃ¥ en uniform distribusjon. Den deler elementene opp i $n$ bÃ¸tter, og siden distribusjonen skal vÃ¦re uniform kommer det ikke til Ã¥ vÃ¦re mange tall i hver bÃ¸tte. Dermed slÃ¥r den sammen alle listene. Bucket Sort er stabil hvis sorteringsalgoritmen den bruker er stabil.

- Best case: $O(n)$
- Worst case: $O(n^2)$
- Average case: $O(n)$

[algdat/BucketSort.scala at main Â· matsjla/algdat](https://github.com/matsjla/algdat/blob/main/src/main/scala/com/supergrecko/dsa/sorting/BucketSort.scala)

## 05. Rotfaste trestrukturer

### ğŸ“šTrÃ¦r

TrÃ¦r er en betegnelse for grafer uten sykluser hvor antall kanter er antall noder minus 1, $|E| = |V| - 1$. Et binÃ¦rtre er et tre hvor hver node har 0-2 barn. Et komplett binÃ¦rtre er et binÃ¦rtre hvor alle lÃ¸vnodene ligger pÃ¥ samme hÃ¸yde med null barn. Et komplett binÃ¦rtre har alltid $2^x$ lÃ¸vnoder nÃ¥r hÃ¸yden er $x$. Derav ser vi ogsÃ¥ at hÃ¸yden av et tre med $n$ noder blir $\lg n$.

![Untitled](05/Untitled%201.png)

Det finnes flere klasser trÃ¦r:

- Et ordnet tre definerer en ordning pÃ¥ barna
- Et posisjonstre har hvert barn en posisjon, og barn kan mangle.
- Et binÃ¦rtre er et posisjonstre hvor hvert barn har to barneposisjoner.

CLRS definerer ikke binÃ¦rtrÃ¦r som trÃ¦r, men vi kan godt tolke dem som ordnede trÃ¦r med ekstra informasjon. Dessuten kan det vÃ¦re hensiktsmessing Ã¥ se pÃ¥ binÃ¦rtrÃ¦r som grafer. I noen tilfeller kan kantene i binÃ¦rtreet ha piler. Det er dermed ogsÃ¥ en directed-acyclic-graph (DAG).

NÃ¥r vi skal traversere treet har vi et par muligheter. I hvilken rekkefÃ¸lge skal vi hÃ¥ndtere den nÃ¥vÃ¦rende noden fÃ¸r vi fortsetter ned treet? Vi har tre muligheter:

- Inorder: besÃ¸k alle nodene til venstre, deretter seg selv, og til slutt de pÃ¥ hÃ¸yre. Dette blir riktig rekkefÃ¸lge for binÃ¦re sÃ¸ketrÃ¦r.
- Preorder: besÃ¸k seg selv, deretter noder til venstre og hÃ¸yre
- Postorder: besÃ¸k noder til venstre og hÃ¸yre, deretter seg selv

Det er kun $InorderTreeWalk$ som er implementert i pensum. Det er dermed Ã¥penbart at kompleksiteten til traversering av et tre med $n$ noder er $\Theta(n)$.

[algdat/InorderWalk.scala at main Â· matsjla/algdat](https://github.com/matsjla/algdat/blob/main/src/main/scala/com/supergrecko/dsa/tree/InorderWalk.scala)

### ğŸ“šBinÃ¦re sÃ¸ktrÃ¦r

BinÃ¦re sÃ¸ketrÃ¦r er funksjonelt sett binÃ¦rsÃ¸k i form av en datastruktur. Barnet til venstre har lavere verdi, og det til hÃ¸yre har en hÃ¸yere verdi. PÃ¥ denne mÃ¥ten blir det veldig lett Ã¥ navigere treet.

Det er ogsÃ¥ greit Ã¥ merke seg at binÃ¦re sÃ¸ketrÃ¦r i pensum ikke hÃ¥ndterer duplikater.

![Untitled](05/Untitled%202.png)

Relevant funksjonalitet pÃ¥ et BinarySearchTree fra pensum er:

- $TreeSearch$ - sÃ¸k for en verdi i treet pÃ¥ kjÃ¸retid $O(\lg n)$, eller $O(1)$ hvis rot er tom eller sÃ¸keverdi.
- $TreeInsert$ - sett inn et nytt tall i treet i $O(\lg n)$ tid, eller $O(1)$ hvis treet er tomt.
- $TreeMinimum$ - finn minimumverdien treet i $O(\lg n)$ tid, eller $O(1)$ hvis treet er tomt.
- $TreeMaximum$ - finn maksverdien i treet i $O(\lg n)$ tid, eller $O(1)$ hvis treet er tomt.
- $TreeSuccessor$ - finn noden med minste verdi som er stÃ¸rre en den gitte noden i $O(\lg n)$ tid.
- $TreeDelete$ - slett et element fra treet i $O(\lg n)$ tid.

[algdat/BinarySearchTree.scala at main Â· matsjla/algdat](https://github.com/matsjla/algdat/blob/main/src/main/scala/com/supergrecko/dsa/tree/BinarySearchTree.scala)

### ğŸ“šHauger

En haug er en datastruktur. I bunn og grunn er en haug en liste eller et array, men vi forestiller oss at haugen er et tre.

Med denne tolkningen kan vi fÃ¥ tilgang til barna eller foreldrene til en node. Hauger er automatisk sÃ¥ balanserte som mulig.

Vi ser ofte pÃ¥ $MaxHeap$ eller $MinHeap$ versjoner av hauger. Dette gir oss en gylden mulighet til Ã¥ implementere prioritetskÃ¸er.

Hauger trenger ikke Ã¥ vÃ¦re sorterte, og siden det finnes eksponensielt mange lovlige rekkefÃ¸lger, gjelder ikke sorteringsgrensen.

Relevant funksjonalitet pÃ¥ en Heap fra pensum er:

- $HeapLeft$: finn posisjonen barnet til venstre til en node i konstant tid $O(1)$.
- $HeapRight$: finn posisjonen barnet til hÃ¸yre til en node i konstant tid $O(1)$.
- $HeapParent$: finn posisjonen foreldrenoden til en node i konstant tid $O(1)$.
- $MaxHeapify$: omplasserer en gitt node og barna til noden slik at den fÃ¸lger max-heap egenskapen i $O(\lg n)$ tid, men konstant $O(1)$ i beste tilfelle.
- $BuildMaxHeap$: bygg et MaxHeap av et array i lineÃ¦r tid $O(n)$.

![Untitled](05/Untitled%203.png)

![Untitled](05/Untitled%204.png)

I tilfellet hvor haugen brukes som en prioritetskÃ¸ Ã¸nsker vi oss fÃ¸lgende funksjonalitet (som ogsÃ¥ er pensum):

- $HeapMax$: finn den stÃ¸rste verdien i haugen i konstant tid $O(1)$.
- $HeapExtractMax$: finn og fjern det stÃ¸rste elementet i haugen i logaritmisk tid $O(\lg n)$.
- $HeapIncreaseKey$: Ã¸k verdien til en nÃ¸kkel i haugen i logaritmisk tid $O(\lg n)$.
- $MaxHeapInsert$: sett inn en ny verdi inn i haugen i logaritmisk tid $O(\lg n)$.

### ğŸ’»HeapSort

Til slutt kan vi bruke disse funksjonene til Ã¥ implementere en sorteringsalgoritme ved hjelp av en haug. Denne algoritmen heter $HeapSort$.

- Best case: $O(n)$
- Worst case: $O(n \lg n)$
- Average case: $O(n \lg n)$

![Untitled](05/Untitled%205.png)

---

## 06. Dynamisk programmering

### ğŸ“šOptimal delstruktur

Optimal delstruktur er en egenskap et problem kan ha, og er nÃ¸dvendig for at problemet kan ha en lÃ¸sning med dynamisk programmering. I tillegg mÃ¥ problemet ha overlappende delinstanser. NÃ¥r et problem har overlappende delinstanser betyr det at flere oppdelinger av problemet inneholder noen av de samme delinstansene.

At et problem har optimal delstruktur betyr at det lÃ¸sningen til problemet kan konstrueres fra optimal lÃ¸sninger pÃ¥ delinstanser av problemet.

Det vil dermed vÃ¦re optimalt Ã¥ cache eller lagre svaret pÃ¥ delinstansene slik at man slipper Ã¥ regne ut lÃ¸sningen nÃ¥r problemet oppstÃ¥r igjen.

Man kan generelt se pÃ¥ dynamisk programmering som et â€œTime-memory tradeoffâ€ hvor vi ofrer minne for Ã¥ lagre dellÃ¸sninger, men i gjengjeld kan vi spare mye tid.

![Untitled](06/Untitled%201.png)

![Untitled](06/Untitled%202.png)

![Untitled](06/Untitled%203.png)

### ğŸ’»Rod Cutting Problem

Stavkapproblemet er et eksempel pÃ¥ et problem som kan lÃ¸ses optimal ved hjelp av dynamisk programmering.

En dum lÃ¸sning pÃ¥ dette problemet kan oppnÃ¥s ved Ã¥ prÃ¸ve alle kombinasjonene rekursivt, og til slutt oppnÃ¥ en kjÃ¸retid pÃ¥ $O(2^n)$. Dette er ikke bra nok!

![Untitled](06/Untitled%204.png)

En optimal lÃ¸sning vil lagre lÃ¸sningene pÃ¥ de forrige verdiene, og det blir dermed mulig Ã¥ slÃ¥ opp lÃ¸sningen pÃ¥ tidligere delproblemer. Grafen nedenfor illustrerer hvor mange kalkulasjoner som spares pÃ¥ denne mÃ¥ten.

![Untitled](06/Untitled%205.png)

### ğŸ“šBottom-up iterasjon vs top-down memoisering

Det finnes to hovedmetoder for Ã¥ programmere en lÃ¸sning som utnytter dynamisk programmering; bottom-up og top-down. Forskjellen pÃ¥ metodene handler om hvordan man designer lÃ¸sningen til Ã¥ utfÃ¸re kallene pÃ¥ delproblemene.

- Top-down: du begynner pÃ¥ hovedproblemet og finner ut hvilke dellÃ¸sninger som er mulige for Ã¥ komme seg dit. Her er det vanlig Ã¥ bruke rekursjon og memoisering av funksjoner.
- Botton-up: du finner ut av hvilke dellÃ¸sninger som kommer til Ã¥ mÃ¥tte lÃ¸ses, og lÃ¸ser disse. Deretter jobber du deg opp mot hovedproblemet og bruker de tidligere lÃ¸sningene. Denne metoden kalles ofte tabulation fordi du fyller gjerne inn dellÃ¸sningene i en tabell.

### ğŸ’»Matrisekjedemultiplikasjon

Matrisekjedemultiplikasjon er en et problem hvor vi Ã¸nsker Ã¥ finne den optimale mÃ¥ten Ã¥ gange sammen en kjede (liste) med matriser av ulik stÃ¸rrelse $\langle A_1, A_2, â€¦ , A_n \rangle$. =

Hvis vi har fire matriser, finnes det fem unike mÃ¥ter Ã¥ gange disse sammen pÃ¥, og basert pÃ¥ stÃ¸rrelsen pÃ¥ hver av matrisene kan antallet kalkulasjoner variere stort.

Hvis vi prÃ¸ver Ã¥ gange sammen matrisene slik som vi vanligvis hadde gjort, kan det hende vi gjÃ¸r unÃ¸dvendig mange kalkulasjoner.

![Untitled](06/Untitled%206.png)

![Fire matriser har 5 unike mÃ¥ter Ã¥ ganges sammen](06/Untitled%207.png)

Fire matriser har 5 unike mÃ¥ter Ã¥ ganges sammen

### ğŸ’»Longest common subsequence

Gitt to sekvenser Ã¸nsker vi Ã¥ finne den lengste subsekvensen som er lik i begge sekvensene.

Hvis vi prÃ¸ver Ã¥ lÃ¸se problemet ved hjelp av brute-force finner vi fort ut at vi fÃ¥r et eksponensielt stort problem som vil vÃ¦re ubrukelig ved store $n$.

![Untitled](06/Untitled%208.png)

### ğŸ’»BinÃ¦re ryggsekkproblemet

Det binÃ¦re rykksekkproblemet handler om at vi skal velge et sett med ting av forskjellig verdi og vekt slik at vi utnytter kapasiteten vÃ¥r $W$ sÃ¥ godt som mulig.

![Untitled](06/Untitled%209.png)

---

## 07. GrÃ¥dighet og stabil matching

### ğŸ“šGrÃ¥dighet

GrÃ¥dighet er en strategi for Ã¥ lÃ¸se problemer ved Ã¥ ta grÃ¥dige valg tidlig, og senere bevise at valget man tok var det optimale. Hittil har vi brukt forskjellige egenskaper ved problemet til Ã¥ bestemme hvilken strategi vi vil bruke for Ã¥ lÃ¸se det.

- Har problemet klare steg som hverken overlapper eller splitter seg? Inkrementelt design.
- Har problemet uavhengige delproblemer? Splitt og hersk.
- Har problemet overlappende problemer? Dynamisk programmering.

I tilfeller hvor dynamisk programmering mÃ¥ foreta valg, kan en grÃ¥dig lÃ¸sning ta et valg sÃ¥ lenge man kan bevise at det var det rette valget.

![Untitled](07/Untitled%201.png)

Det er ikke mulig Ã¥ bruke grÃ¥dighet i alle lÃ¸sninger. Problemet (og delproblemene) mÃ¥ oppfylle grÃ¥dighetsegenskapen:

- Den globale optimale lÃ¸sningen kan nÃ¥s ved Ã¥ ta et optimalt lokalt valg.
- Ved Ã¥ ta et optimalt lokalt valg, mÃ¥ vi ikke ha eliminert alle mulige optimale lÃ¸sninger

Dessuten vil det ikke gi mening Ã¥ bruke grÃ¥dighet pÃ¥ et problem uten optimal delstruktur.

![Untitled](07/Untitled%202.png)

### ğŸ’»Kontinuerlige ryggsekkproblemet

Det kontinuerlige ryggsegg bygger videre pÃ¥ det binÃ¦re ryggsekkproblemet. Forskjeller er at du nÃ¥ har mulighet til Ã¥ velge antallet av verdien istdenfor Ã¥ enten ta eller forkaste verdien.

LÃ¸sningen til det kontinuerlige ryggsekkproblemet er Ã¥ ta sÃ¥ mye av det med hÃ¸yest verdi som mulig, fordi et annet valg vil aldri klare Ã¥ ta igjen det grÃ¥digste.

![Untitled](07/Untitled%203.png)

### ğŸ’»Aktivitetsutvalg

Aktivitetsutvalgproblemet handler om Ã¥ velge de beste tidene pÃ¥ en timeplan av et utvalg med aktiviteter slik at timeplanen er sÃ¥ full som mulig. Dette kan for eksempel vÃ¦re planlegging av en forelesningsal med forskjellige forelesninger og andre aktiviteter som har lyst til Ã¥ bruke salen. Da gjelder det Ã¥ velge de beste aktivitetene slik at salen er brukt sÃ¥ mye som mulig.

![Untitled](07/Untitled%204.png)

LÃ¸sningen vil vÃ¦re Ã¥ velge aktiviteten som slutter tidligst.

### ğŸ’»Huffman

Huffman er en komprimeringsalgoritme som lager en enkoding basert pÃ¥ frekvensen til bokstavene eller tegnene i teksten. MÃ¥let med Huffman enkodingen er Ã¥ lage en enkoding med kortest kodelengde. Dette gjÃ¸res ved Ã¥ bygge et Huffman-tre og deretter traversere treet for Ã¥ generere enkodingen basert pÃ¥ inputen.

![Untitled](07/Untitled%205.png)

Det optimalet treet gir bokstaver som blir brukt minst de lengste kodene.

### ğŸ’»Gale-Shapley

Gale-Shapley er en stabil matchingalgoritme som matcher kvinner og menn (eller andre entiteter som studenter og skoler). En matchingalgoritme er stabil om det ikke finnes noen blokkerende par. Et blokkerende par er et par som foretrekker hverandre ovenfor parteren de har blitt tildelt.

Gale-Shapley er designet slik at matchingen er optimal for kvinnene, og pessimal for mennene.

---

## 08. Traversering av grafer

### ğŸ“šGrafrepresentasjoner

Det finnes flere mÃ¥ter Ã¥ representere en graf i minne. I pensum er nabomatriser og nabolister i fokus. Representasjonene har noen forskjeller, og man sier ofte at nabomatriser er raskere, men de tar mer plass i minne.

Nabomatriser har en fordel siden det er mulig Ã¥ slÃ¥ opp kanter i grafen direkte, men i hovedsak bruker vi nabolister. Dessuten er det andre faktorer som veier inn pÃ¥ hvilken representasjon vi bruker, og som nevnt finnes det mange andre representasjoner.

![Untitled](08/Untitled%201.png)

![Untitled](08/Untitled%202.png)

Nabomatriser egner seg til direkte oppslag. Nabolister egner seg til traversering. Nabolister tar ogsÃ¥ mindre plass dersom grafen har fÃ¥ kanter â€“ men ikke ellers!

### ğŸ“šTraversering av grafer

Traversering av en graf kan kjÃ¸res pÃ¥ mange forskjellige mÃ¥ter, og pensum har tidligere vÃ¦rt innom traversering av et trÃ¦r. MÃ¥let med Ã¥ traversere en graf er Ã¥ unngÃ¥ Ã¥ besÃ¸ke samme node to ganger, selv om grafen har sykler. Pensum nevner to enkle mÃ¥ter Ã¥ traversere grafer pÃ¥;

- Slett noden fra grafen
- Marker noder i grafen med farger, nÃ¥vÃ¦rende node som grÃ¥, besÃ¸kte som svart, og ubesÃ¸kte som hvite.

![Untitled](08/Untitled%203.png)

### ğŸ’»Depth-first search

Depth-first search er en traverseringsalgoritme som gÃ¥r sÃ¥ dypt igjennom kantene i grafen som mulig fÃ¸r den backtracker og fortsetter i bredden. Dette skjer fordi depth-first search besÃ¸ker alle nodene i grafen, og for hver node besÃ¸ker alle nodene som er koblet til den nÃ¥vÃ¦rende noden. PÃ¥ denne mÃ¥tten vil den alltid nÃ¥ â€œbunnenâ€ fÃ¸r den fortsetter i bredden.

- Kompleksitet pÃ¥ $O(V + E)$ siden algoritmen besÃ¸ker hver node og hver kant.

I depth-first search er det vanlig Ã¥ klassifisere nodene. Vi har fÃ¸lgende klasser:

- Tre-kanter: kanter som er i dybde-fÃ¸rst skogen, altsÃ¥ nÃ¥r du mÃ¸ter en hvit node.
- Bakoverkanter: kanter til en forhjenger i skogen, altsÃ¥ nÃ¥r du mÃ¸ter en grÃ¥ node.
- Forkanter: kanter som ikke er en del av DFS-skogen.
- Krysskanter: kanter som er ikke har noen anscestor/descendant relasjon

Dessuten er det interessant Ã¥ merke seg at i dynamisk programmering utfÃ¸rer vi implisitt DFS pÃ¥ delproblemene.

![Untitled](08/Untitled%204.png)

### ğŸ’»Breadth-first search

Breadth-first search er en traverseringsalgoritme som gÃ¥r sÃ¥ bredt igjennom kantene i grafen som mulig fÃ¸r den forsetter i dybden. Den kan ogsÃ¥ brukes til Ã¥ finne korteste vei fra en node til alle. Breadth-first search bruker en kÃ¸ over noder den Ã¸nsker Ã¥ besÃ¸ke, og kan pÃ¥ denne mÃ¥ten sikre seg at den sÃ¸ker i bredden fÃ¸rst.

- Kompleksitet pÃ¥ $O(V + E)$ siden algoritmen besÃ¸ker hver node og hver kant.

### ğŸ’»Topological sort

Topological sort er en traverseingsmetode pÃ¥ en DAG som gir nodene en viss rekkefÃ¸lge; foreldre kommer fÃ¸r barn.

Det finnes flere mÃ¥ter Ã¥ implementere topological sort pÃ¥, og forelesning tar en kjapp titt pÃ¥ en versjon som fjerner â€œsourcesâ€ fra grafen. En DAG har minst en â€œsourceâ€, en node uten noen inn-kanter.

Typisk implementasjon av topological sort bruker en algoritme lik DFS med en Stack. Her kjÃ¸rer man DFS over grafen, deretter sortere rangere etter synkende finish-tid.

- Kompleksitet pÃ¥ $O(V + E)$ siden algoritmen besÃ¸ker hver node og hver kant.

---

## 09. Minimale spenntrÃ¦r

### ğŸ“šDisjunkte mengder

Disjunkte mengder kommer til nytte nÃ¥r vi Ã¸nsker Ã¥ finne spanntrÃ¦r i en graf. Her kommer litt nyttig terminologi.

- Delgraf: graf som bestÃ¥r av en delmengde av nodene og kantene til den opprinnelige grafen.
- Spenngraf: en dekkende delgraf, graf med samme nodesett som originalgrafen. (uvanlig begrep pÃ¥ norsk)
- Spennskog: en asyklisk spenngraf. (uvanlig begrep pÃ¥ norsk)
- SpenntrÃ¦r: en sammenhengende spennskog, altsÃ¥ $|E| = |V| - 1$.

Disjunkte mengder er en effektiv mÃ¥te Ã¥ gruppere noder inn i sammenkoblede komponenter ved Ã¥ introdusere en â€œparentâ€ til hver node. Noden pÃ¥ topp peker deretter pÃ¥ seg selv.

![Untitled](09/Untitled%201.png)

I tillegg har hver node en rang som beskriver hvor hÃ¸yt opp i hierarkiet den ligger. I andre tilfeller kan denne komme til nytte, men i tilfellet med minimale spanntrÃ¦r Ã¸nsker vi kun Ã¥ gruppere nodene inn i komponenter. Denne rangen brukes for Ã¥ merge to komponenter (for Ã¥ bestemme hvilken node som dominerer). Relevant funksjonalitet fra pensom er:

- $MakeSet$ - Lag et sett med ett element av en node. Setter parent til seg selv, og rang til 0. (I konstant tid $O(1)$)
- $FindSet$ - Finn representanten (Ã¸verste parent) til mengden som inneholder den en gitt node, ved amortisert analyse i $O(\alpha(n))$ tid.
- $Union$ - Merge to sett sammen, hvor settet med hÃ¸yest rang pÃ¥ Ã¸verste parent dominerer den andre. Hvis begge er like, velges en vilkÃ¥rlig.
- $Link$ - Lenker et sett til et annet, hvor settet med hÃ¸yest rang pÃ¥ Ã¸verste parent dominerer den andre. (Kalt av $Union$)

![Untitled](09/Untitled%202.png)

- $ConnectedComponents$ - Danne sammenkoblede komponenter av en graf. Dette kan gjÃ¸res i tilnÃ¦rmet $O(V + E)$ tid (variasjon, basert pÃ¥ amortisert kjÃ¸retid av $FindSet$).
- $SameComponents$ - Finn ut om to noder er i samme komponent, i tilnÃ¦rmet $O(1)$ tid (variasjon, basert pÃ¥ amortisert kjÃ¸retid av $FindSet$).

### ğŸ’»Generisk MST

Kanter kan ha vekter som betyr at vi til tider Ã¸nsker minimale eller maksimale spenntrÃ¦r. Realistisk eksempel kan vÃ¦re bygging av veier mellom byer. Et minimalt spenntre er spenntreet som knytter sammen nodene med minst vekt mulig, og er et praktfult eksempel pÃ¥ en grÃ¥dig algoritme.

![Untitled](09/Untitled%203.png)

![Untitled](09/Untitled%204.png)

Eksempelet i forelesning bruker kantkontraksjon. Her vil hvert alternativ gi en ny, mindre delinstans. Dette passer godt til dynamisk programmering, men det er mye enklere Ã¥ lÃ¸se problemet pÃ¥ grÃ¥dig vis.

![Untitled](09/Untitled%205.png)

Pensum bygger videre pÃ¥ dette prinsippet gjennom to algoritmer som opererer pÃ¥ litt forskjellig vis:

- Kruskal: en kant med minimal vekt blant de gjenvÃ¦rende er trygg sÃ¥ lenge den ikke danner sykler
- Prim: bygger ett tre gradvis; en lett kant over snittet rundt treet er alltid trygg

### ğŸ’»Kruskals algoritme

Kruskals algoritme fungerer ved Ã¥ splitte opp grafen i to skoger som man jobber med; et tre man bygger til et MST, og den andre som er en skog av disjunkt mengder.

![Untitled](09/Untitled%206.png)

![Untitled](09/Untitled%207.png)

### ğŸ’»Prims algoritme

Prims algoritme kan implementeres pÃ¥ forskjellige mÃ¥ter, men pensum dekker en metode lik BFS ved hjelp av en min-prioritets-kÃ¸. Prioriteten er vekten pÃ¥ den letteste kanten mellom noden og treet.

![Untitled](09/Untitled%208.png)

![Untitled](09/Untitled%209.png)

## 10. Korteste vei fra Ã©n til alle

NÃ¥r vi Ã¸nsker Ã¥ finne korteste vei fra Ã©n til alle, Ã¸nsker vi Ã¥ finne de korteste stiene fra en gitt node til alle andre noder. I tillegg vet vi at:

- En **enkel sti** er en sti uten sykler
- En **kortest sti** er alltid **enkel**
- Hvis det finnes en negativ sykel i veien, finnes det ingen **kortest sti**

  - Det finnes fortsatt en **kortest enklest sti** (utenfor syklen)
  - Ã… finne denne er et ulÃ¸sp NP-hardt problem.

  ![Untitled](10/Untitled%201.png)

![Untitled](10/Untitled%202.png)

### ğŸ’»Dag-Shortest-Paths

Dette er en simpel algoritme for Ã¥ finne de korteste stiene til alle nodene fra en gitt node gitt at det ikke finnes noen sykler (positive eller negative) i nodene som kan nÃ¥s fra startnoden. Dette er fordi algoritmen bruker en topologisk sortering nÃ¥r den traverserer nodene i grafen.

![Untitled](10/Untitled%203.png)

![Untitled](10/Untitled%204.png)

### ğŸ’»Bellman-Ford

Hvis vi har sykler grafen som kan nÃ¥s fra startnoden kan dette lÃ¸ses pÃ¥ flere mÃ¥ter. Vi kan holde styr pÃ¥ hvilke noder som endres ved iterasjon, eller oppdatere alle kantene til ingenting endrer seg.

Teoretisk sett, skal ingenting endre seg etter $|V - 1|$ iterasjoner, siden hver node i grafen skal ha blitt besÃ¸kt sÃ¥ langt. Hvis ting endrer seg etter sÃ¥ mange iterasjoner mÃ¥ det finnes en negativ sykel.

![Untitled](10/Untitled%205.png)

![Untitled](10/Untitled%206.png)

### ğŸ’»Dijkstras algoritme

Dijkstras algoritme tar utgangspunkt i at noden med lavest estimat mÃ¥ vÃ¦re ferdigkalkulert. Den eneste mÃ¥ten vekten til stien kan bli bedre er om vi hadde negative kanter. Dijkstras algoritme fungerer dermed ikke hvis grafen har negative kanter.

![Untitled](10/Untitled%207.png)

![Untitled](10/Untitled%208.png)

KjÃ¸retiden kan videre optimaliseres ved Ã¥ bruke fibonacci-heaps.

## 11. Korteste vei fra alle til alle

Trekantulikheten kan komme til nytte for Ã¥ forstÃ¥ hvordan den korteste veien kan finnes. Hvis det finnes en snarvei fra $i \to j$ som gÃ¥r gjennom $k$, sÃ¥ har vi ikke enna funnet den korteste veien fra $i \to j$.

$$
\overline{AC} \leq \overline{AB} + \overline{BC}
$$

![Untitled](11/Untitled%201.png)

$Relax$ prosedyren vÃ¥r sjekker om denne snarveien er kortere, og reparerer/gjeninnfÃ¸rer trekantulikheten. NÃ¥r vi er ferdige, kan ikke $i \to k \to j$ fortsatt vÃ¦re en snarvei. ???

1. $\omega(k, j) + \delta(i, k) - \delta(i, j) \geq 0$
2. Hvis $i \to k$ og $k \to j$ finnes, finnes $i \to j$
3. $\delta(i, k) + \delta(k, j) \geq \delta(i, j)$

![$Relax(u, v, \omega)$](11/Untitled%202.png)

$Relax(u, v, \omega)$

### ğŸ’»Johnsons algoritme

Johnsons algoritme er en simpel mÃ¥te Ã¥ finne korteste vei fra alle til alle i en graf. Den bruker i all hovedsak Dijkstras algoritme for Ã¥ finne korteste vei fra Ã©n til alle over alle nodene i grafen.

![Det er ogsÃ¥ vanlig Ã¥ returnere en forgjengermatrise $\Pi = (\pi_{ij})$](11/Untitled%203.png)

Det er ogsÃ¥ vanlig Ã¥ returnere en forgjengermatrise $\Pi = (\pi_{ij})$

Ettersom Dijkstras algoritme ikke fungerer med negative kanter, mÃ¥ vi balansere alle kantene i grafen helt til det ikke finnes negative kanter. Etter utregningen vÃ¥r, kan vi omjustere igjen for Ã¥ finne de originale vektene.

> Beskrivelse bruker Turing-reduksjoner, forelesning 13-14 begrenser dette til Karp-reduksjoner.

![Untitled](11/Untitled%204.png)

NÃ¥r vi skal Ã¸ke kantvektene, kan det virke intuitivt Ã¥ Ã¸ke alle kantene med like mye, men stier med mange kanter taper pÃ¥ denne Ã¸kningen. Vi vil heller gi hver node en tilordnet verdi $h(node)$. Vekten $\omega(v, u)$ Ã¸kes med differansen $h(u) - h(v)$. PÃ¥ denne mÃ¥te vil positive og negative ledd bortsett fra det fÃ¸rste og det siste oppheve hverandre.

![Untitled](11/Untitled%205.png)

For Ã¥ sikre oss at vi ikke ender opp med noen negative vekter, kan vi igjen huske pÃ¥ trekantulikheten som forteller oss at $\omega(k, j) + \delta(i, k) - \delta(i, j) \geq 0$. Vi kan rett og sslett legge til en ny node.

Algoritmen kjÃ¸rer ved Ã¥ velge en tilfeldig startnode $s$ fra grafen, og deretter kjÃ¸re $BellmannFord$ for Ã¥ finne ut om grafen har noen negative sykler. Deretter definerer vi differansefunksjonen $h(v)$ for enhver node basert pÃ¥ distansene i resultatet fra $BellmanFord$. Vi lager deretter $\hat{w}(u, v)$ med den nye vektingen til hver kant, justert med differansen $h(u) - h(v)$. Deretter kjÃ¸rer vi $Dijkstra$ pÃ¥ hver node i grafen.

![Untitled](11/Untitled%206.png)

### ğŸ’»â€Matriseproduktâ€

Denne algoritmen har egentlig ingen relasjon til matriseprodukt Ã¥ gjÃ¸re, bortsett fra at den ene prosedyren ligner litt.

![Untitled](11/Untitled%207.png)

I denne algoritmen innfÃ¸rer vi et parameter $r$ som er antall kanter vi har lov til Ã¥ besÃ¸ke pÃ¥ stien fra $i \to j$. Vi antar induktivt at lÃ¸st det for alle avstander $r - 1$, og kan dermed finne $l^{(r)}_{ij} = min_k l^{(r - 1)}_{ij} + \omega(k, j)$.

Induktivt vil vi dermed finne den korteste stien ved dette â€œpunktetâ€ i grafen.

![Untitled](11/Untitled%208.png)

![Untitled](11/Untitled%209.png)

![Untitled](11/Untitled%2010.png)

Desverre er kjÃ¸retiden pÃ¥ denne algoritmen $O(n^4)$. (Kanskje derfor den heter â€œSlowâ€). Bedre versjon bruker repeated squaring.

Dette resulterer i en bedre kjÃ¸retid pÃ¥ $\Theta(n^3 \lg n)$

![Untitled](11/Untitled%2011.png)

### ğŸ“šTransitiv lukning

Gitt en graf, sÃ¥ Ã¸nsker vi Ã¥ finne ut om enhver node $i$ kan nÃ¥s fra alle de andre nodene. Her kan vi igjen lÃ¸se induktivt ved Ã¥ bruke trekantulikheten.

Finnes det en sti $i \to j$ som fÃ¥r gÃ¥ innom nodene ${1, â€¦, n}$, dvs. alle?

![Untitled](11/Untitled%2012.png)

![Untitled](11/Untitled%2013.png)

![Untitled](11/Untitled%2014.png)

For enhver vei fra $i \to j$, kan vi enten velge den direkte strekningen $i \to j$, eller sÃ¥ kan vi velge $i \to j \to k$.

![Untitled](11/Untitled%2015.png)

Forenklet implementasjon av algoritmen til hÃ¸yre kan implementeres med en enkel tabell. Man risikerer Ã¥ gÃ¥ innom samme node flere ganger, men det betyr bare at vi lÃ¸ste et senere delproblem tidligere.

Dessuten om det finnes stier med sykler, sÃ¥ finnes det ogsÃ¥ en uten.

![Untitled](11/Untitled%2016.png)

![Untitled](11/Untitled%2017.png)

![Untitled](11/Untitled%2018.png)

Totalt blir kjÃ¸retiden pÃ¥ $TransitiveClosure$ $\Theta(n^3)$. Analyse fra forelesning:

![Untitled](11/Untitled%2019.png)

### ğŸ’»Floyd-Warshall

Vi har allerede sett pÃ¥ mÃ¥ter vi kan lÃ¸se alle til alle problemet med Dijkstras algoritme over hver node, men vi vil gjÃ¸re det mulig Ã¥ kjÃ¸re algoritmen flere typer grafer.

![Untitled](11/Untitled%2020.png)

Ved Ã¥ bygge videre pÃ¥ prinsippet fra transitiv lukning kan vi bruke samme mÃ¥te pÃ¥ Ã¥ velge den korteste av de to veiene som forgjenger til en hver node. Det er akkurat dette $FloydWarshall$ gjÃ¸r.

![Untitled](11/Untitled%2021.png)

![Untitled](11/Untitled%2022.png)

![Untitled](11/Untitled%2023.png)

![Untitled](11/Untitled%2024.png)

![Untitled](11/Untitled%2025.png)

![Untitled](11/Untitled%2026.png)

Dette resulterer i en total kjÃ¸retid pÃ¥ $\Theta(n^3)$. I tillegg kan det vÃ¦re nyttig Ã¥ kunne printene veiene fra en node til en annen. Dette kan enkelt gjÃ¸res ved Ã¥ besÃ¸ke forgjengermatrisen.

![Untitled](11/Untitled%2027.png)

## 12. Maksimal flyt

### ğŸ“šProblemet

Maksimal flyt er et problem pÃ¥ en rettet graf $G = (V, E)$. Et flytnett er grafen hvor flyten kan flyte igjennom, og den har fÃ¸lgende kriterier:

- Hver kant har en kapasitet $c(u, v) \geq 0$
- Det finnes ihvertfall en kilde, og en sluk $s, t \in V$
- For en hver $v \in V \implies s \to v \to t$
- Grafen har ingen self-loops
- $(u, v) \in E \implies (v, u) \not\in E$
- $(u, v) \not\in E \implies c(u, v) = 0$

Den resulterende flyten er en funksjon $f : V \times V \to \R$. Flyten pÃ¥ en kant kan aldri vÃ¦re stÃ¸rre en kapasiteten pÃ¥ kanten, altsÃ¥:

- $0 \leq f(u, v) \leq c(u, v)$
- For en kant $u \neq s, t$, gjelder $\sum_v f(u, v) = \sum_v(v, u)$

Flytverdien $|f| = \sum_v f(s, v) - \sum_v f(v, s)$ er flyten som kan nÃ¥s fra source til en gitt node.

Hvis du har flere kilder og sluker, legger man til en super-kilde og en super-sluk

![Untitled](12/Untitled%201.png)

Det minimale snittet er gitt etter Ã¥ ha kjÃ¸rt maksimal flyt pÃ¥ en graf. Den skaper en partisjon i flytnettet etter at sluket til grafen ikke kan nÃ¥s lengre. Snittet i flytnettet er en partisjon $(S, T)$ av $V$. $s \in S$ og $t \in T$.

Netto-flyten beskriver flyten som mellom partisjonen.

![Untitled](12/Untitled%202.png)

$$
f(S, T) = \sum_{u \in S}\sum_{v \in S}f(u, v) - \sum_{u \in T}\sum_{v \in T}f(u, v)
$$

Kapasiteten blir $c(S, T) = \sum_{u \in S}\sum_{v \in T} c(u, v)$.

Til slutt beskriver Lemma 24.4: $f(S, T) = |f|$. Bevis for dette krever en del utregning, men er ganske rett frem. Videre Corollary 24.5: $|f| \leq c(S, T)$.

![Untitled](12/Untitled%203.png)

![Untitled](12/Untitled%204.png)

Maksimal flyt fÃ¸rer dermed til det minste snittet. Videre forteller helltallsteoremet (24.10) at for heltallskapsiteter $c(u, v) \in \N$ gir $FordFulkerson$ og andre flytalgoritmer basert pÃ¥ $FordFulkerson$ en heltallsflyt $|f| \in \N$.

### ğŸ“šReduksjon til flyt

Flyt kan nesten regnes som en designmetode siden vi kan redusere mange problemer til flytproblemet. Pensum ser pÃ¥ et par mÃ¥ter Ã¥ bryte ned stÃ¸rre problemer til flytproblemer. Blant annet:

- Biparitt matching, gjÃ¸r om hver donor og mottaker til sink/source hvor kantene har en kapasitet pÃ¥ 1, deretter lag en supersource/supersink som vanlig. Greit Ã¥ notere seg at $EdmondsKarp$ er ikke den mest effektive mÃ¥te Ã¥ gjÃ¸re dette pÃ¥. I rekonstruksjonen representerer kanter med flyt matchingen.

  ![Untitled](12/Untitled%205.png)

- Legekontor hvor man Ã¸nsker minst Ã©n lege pÃ¥ vakt hver dag i feriene, men hver lege skal jobbe maks $c$ feriedager totalt, og maks en gang per ferie. Denne lÃ¸ses ved Ã¥ gjÃ¸re legene om til sources med kapasitet $c$, og lag en source med kapasitet 1 for hver feriedag.
- Bildesegmentering for Ã¥ separere forgrunn og bakgrunn kan lÃ¸ses ved Ã¥ gjÃ¸re hver piksel til en node, og kapasiteten fra kilde tilsvarende â€œforgrunnsaktighetâ€ og kapasitet til sluk â€œbakgrunnsaktighetâ€. Kapaisteten mellom noder tilsvarer hvor like de er. Her vil det minimale snittet skille forgrunn fra bakgrunn.
- Veisperring kan lÃ¸ses ved Ã¥ gjÃ¸re Ã¥stedet til source, og hvert sted man kan sette opp sperringer til sinks med kapasitet basert pÃ¥ hvor fort forbryterne kan bevege seg.

### ğŸ’»Biparitt matching

Designmetoden som brukes for biparitt matching baserer seg pÃ¥ Ã¥ gjentatte ganger konstruere et nytt, enklere problem, som gir en forbedring av den opprinnelige instansen.

Vi Ã¸ker matchingen ved Ã¥ finne stier fra venstre til hÃ¸yre (i en liggende graf). Kanter som brukes i matchingen blir snudd baklengs slik at det er mulig Ã¥ endre pÃ¥ de hvis det viser seg at det er mer optimalt senere.

Hvis vi for eksempel finner ut at en donor kan kun matches med fÃ¸rste resipient (men vi har allerede gitt fÃ¸rste resipent en donor), kan vi bruke dette til Ã¥ oppheve matchingen.

![Untitled](12/Untitled%206.png)

### ğŸ“šIdeer

Det samme gjelder for flyt. Vi har kanskje fÃ¥tt frem Ã©n enhet, men hvis vi har en forÃ¸kende sti (sti som gir bedre resultat en nÃ¥vÃ¦rende), har vi lyst til Ã¥ sende tilbake enheten. Den originale enheten (eller flyten) sendes tilbake hvor den kom fra, og sÃ¥ finner vi en ny vei frem. Da fÃ¥r vi igjen en forÃ¸kende sti, og flyter fremover, og opphever den bakover til flyten nÃ¥r der den kom fra.

![Untitled](12/Untitled%207.png)

Et restnett (residual network) er et nettverk likt flytnettet, bortsett fra at kantene mellom nodene er fremoverkanter ved ledig kapasitet, og bakoverkanter ved flyt.

En forÃ¸kende sti (augmenting path) er en sti fra kilde til sluk i restnettet. Langs fremoverkanter kan flyten Ã¸kes, og langs bakoverkanter kan flyten omdirigeres. Det er altsÃ¥ en sti hvor den totale flyten kan Ã¸kes.

Det er dermed nyttig Ã¥ finne ut hva potensialet (eller restkapasiteten) mellom to noder er. Hvor mye kan vi Ã¸ke flyten fra $u$ til $v$?

![Untitled](12/Untitled%208.png)

### ğŸ’»Ford-Fulkerson

$FordFulkerson$ er en generell metode for Ã¥ finne den maksimale flyten gjennom et nettverk. En versjon av $FordFulkerson$ er $EdmondKarp$ som bruker en BFS. Ford-Fulkersom fungerer ved Ã¥:

1. Finn forÃ¸kende stier sÃ¥ lenge det gÃ¥r
2. Deretter er flyten sÃ¥ stor den kan bli.

Dette gjÃ¸res som regel ved Ã¥ finne den forÃ¸kende stien, deretter finner man flaskehalsen i stien, og oppdaterer flyt langs stien med denne verdien.

![Untitled](12/Untitled%209.png)

En mer konkret beskrivelse av algoritmen finnes i pensum:

![Untitled](12/Untitled%2010.png)

Alternativt kan man flette inn BFS ved Ã¥ finne flaskehalser underveis. Hold deretter styr pÃ¥ hvor mye flyt vi fÃ¥r frem til hver node, og traverser kun noder vi ikke har nÃ¥dd frem til enda.

Denne implementasjonen stÃ¥r ikke i boka, og det er ikke krav Ã¥ kunne denne i detalj.

![Untitled](12/Untitled%2011.png)

![Untitled](12/Untitled%2012.png)

![Untitled](12/Untitled%2013.png)

NÃ¥r vi hr kjÃ¸rt Ford-Fulkerson kan vi finne det minimale snittet direkte fra resultatet. Forelesningsnotatet inneholder mer om bevis pÃ¥ korrekthet.

KjÃ¸retiden pÃ¥ Ford-Fulkerson kan gÃ¥ ille hvis vi ikke bruker BFS. En graf som ser slik ut, vil kunne sende flyten fra og tilbake pÃ¥ noden med 1 kapasitet flere millioner ganger.

Med irrasjonale kapasiteter kan vi ikke garantere at Ford-Fulkerson terminerer,

Hvis vi bruker BFS, vil fÃ¥r vi en kjÃ¸retid pÃ¥ $O(VE^2)$ for Ã¥ finne forÃ¸kende stier.

Pensum har en lengre forklaring pÃ¥ hvorfor vi fÃ¥r $O(VE)$ i indre lÃ¸kke.

![Untitled](12/Untitled%2014.png)

[https://folk.idi.ntnu.no/mlh/algkon/flow.pdf](https://folk.idi.ntnu.no/mlh/algkon/flow.pdf)

## 13. NP-kompletthet

### ğŸ“šReduksjon

I denne forelesningen betegner reduksjoner Karp-reduksjoner, eller â€œmany-oneâ€ reduksjoner som tar polynomisk lang tid.

Vi Ã¸nsker Ã¥ beskrive problemer sin vanskelighetsgrad. Det er teoretisk mulig Ã¥ finne absolutt vanskeliggrad av et problem, men det er lite nyttig, sÃ¥ vi vil heller sammenligne og kategorisere problemer.

Et eksempel pÃ¥ en Karp-reduksjon fra forelesning:

> Gitt en kiste $A$, og en kiste $B$ hvor nÃ¸kkelen til $A$ ligger inni $B$. Ved reduksjon er det dermed Ã¥penbart at det ikke kan vÃ¦re vanskeligere Ã¥ Ã¥pne $A$ enn det er Ã¥ Ã¥pne $B$.

Hva â€œminst like vanskeligâ€ betyr kommer an pÃ¥ reduksjonene, men i vÃ¥rt tilfelle bruker vi polynomiske reduksjoner som betyr at problemene kan lÃ¸ses i polynomisk tid.

Hvis vi Ã¸nsker Ã¥ lÃ¸se problemet â€œhar en graf en kort sti mellom node $u$ og $v$â€ kan vi redusere dette ned til Ã¥ finne korteste sti mellom nodene. Det er dermed Ã¥penbart at Ã¥ finne ut om det finnes en kort sti, er minst like vanskelig som Ã¥ finne korteste sti.

![Untitled](13/Untitled%201.png)

![Untitled](13/Untitled%202.png)

![Untitled](13/Untitled%203.png)

Hvis vi har et problem $\alpha$ som vi kan polynomisk redusere til et annet problem $\beta$ som ogsÃ¥ er polynomisk, er det Ã¥penbart at vi kan bruke lÃ¸sningen $B(\beta)$ for Ã¥ lage en lÃ¸sning $A(\alpha)$.

![Untitled](13/Untitled%204.png)

Vi kan redusere Hamilton-sykel problemet ned til Long-Path problemet i polynomisk tid, sÃ¥ det betyr at $HamCycle$ kan umulig vÃ¦re vanskeligere enn $LongPath$, og $LongPath$ er minst like vanskelig som $HamCycle$.

PÃ¥ denne mÃ¥ten kan vi ogsÃ¥ redusere perfekt matching til $HamCycle$, og det betyr at $Match$ kan umulig vÃ¦re vanskeligere enn $HamCycle$, og dermed ikke vanskeligere enn $LongPath$. Det eneste problemet er at vi ikke vet hvordan vi kan finne en lÃ¸sning pÃ¥ $LongPath$.

### ğŸ“šVerifikasjon

Beslutningsproblemer er problemer vi kan verifiserer et sertifikat for et ja-svar. Hvis vi lurer pÃ¥ om $X$ finnes, kan vi bruke verifikasjonsfunksjonen til Ã¥ sjekke enhver mulig $X$ for en instans. Verifikasjonsmetoden i dette tilfellet kjÃ¸rer i polynomisk tid slik at det er mulig Ã¥ verifisere det. Hvis svaret er â€œneiâ€, sÃ¥ har vi ingen sertifkat for problemet, ingenting Ã¥ verifisere.

![Untitled](13/Untitled%205.png)

Et beslutningsproblem stiller rett og slett spÃ¸rsmÃ¥let â€œfinnes det et sertifikat for at svaret er ja?â€ for et slikt sertifikat skal finnes hvis, og bare hvis svaret er â€œjaâ€.

- Problemklassen **P** er problemer som kan lÃ¸ses i polynomisk tid.
- Problemklassen **NP** har ja-svar sertifikater som kan sjekkes i polynomisk tid. Problemer i **NP** som ogsÃ¥ er i **P** er selvfÃ¸lgelig lÃ¸sbare i polynomisk tid.
- Problemklassen **NPC** kan vi ikke lÃ¸se, men vi kan verifisere de i polynomisk tid. NP-komplette problemer er subsettet av **NP** som alle andre **NP** problemer kan reduseres til i polynomisk tid.
- Problemklassen **NPH** kan ikke lÃ¸ses eller verifiseres, men de som ogsÃ¥ er i **NPC** kan selvfÃ¸lgelig sjekkes i polynomisk tid. Dette er problemer som er minst like vanskelige som **NPC**, men de trenger ikke Ã¥ vÃ¦re i **NP**, og trenger ikke Ã¥ vÃ¦re beslutningsproblemer.
- Problemklassen **co-NP** har nei-svar som kan sjekkes i polynomisk tid.

Dette betyr at hvis man finner et bevis for Ã¥ lÃ¸se et NP-komplett problem, kan man lÃ¸se alle andre NP problemer.

Det er ogsÃ¥ praktisk Ã¥ se pÃ¥ beslutningsproblemene som mengder av instanser (bitstrenger) der svaret er ja. En slik mengde er et formelt sprÃ¥k.

![Untitled](13/Untitled%206.png)

[What are the differences between NP, NP-Complete and NP-Hard?](https://stackoverflow.com/a/19510170/14996499)

### ğŸ“šKompletthet

Vi har et univers av problemer og vi kan sammenligne dem. Det gir faktisk opphav til et sett med maksimalt vanskelige problemer. Disse kalles komplette for klassen NP under polynomiske reduksjoner.

Hvis vi har en mengde med kister, og en ekstra kiste $K$ som har nÃ¸kkelen til alle de andre, sÃ¥ kan vi redusere Ã¥pne alle kistene til Ã¥pne $K$. Kompletthet er en universalnÃ¸kkel for alle problemene. Finner vi nÃ¸kkelen her, kan vi lÃ¸se alle de andre problemene.

Slik ser vi for oss at hierarkiet til problemene er, selv om det ikke er helt mulig Ã¥ vite. Det finnes ogsÃ¥ mange andre klasser med problemer, og det er mange mulige scenarier. Problemet er at ingen vet hva som stemmer.

Se slides for bedre beskrivelse

![Untitled](13/Untitled%207.png)

Hvis det har seg slik at $P = NP$, kan vi ikke bare svare pÃ¥ om det finnes et sertifikat, men vi kan rekonstruere sertifikatet. Med andre ord, kan vi lÃ¸se beslutningsproblemer, sÃ¥ kan vi lÃ¸se sÃ¸keproblemer ogsÃ¥, og finne gyldig output.

### ğŸ’»Oppfyllbarhet

Oppfyllbarhetsproblemet var det fÃ¸rste problemet som ble bevist til Ã¥ vÃ¦re **NPC**. Det gÃ¥r ut pÃ¥ Ã¥ finne ut om det er mulig at en logisk krets sine inputs kan bestemme om det er mulig at output er 1.

![Untitled](13/Untitled%208.png)

![Untitled](13/Untitled%209.png)

![Untitled](13/Untitled%2010.png)
